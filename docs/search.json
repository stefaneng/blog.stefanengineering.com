[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a Ph.D student in Biostatistics at the University of Michigan. Sometimes I like to run for a really long time."
  },
  {
    "objectID": "posts/2018-11-05-cloze-deletion-detection/index.html",
    "href": "posts/2018-11-05-cloze-deletion-detection/index.html",
    "title": "Cloze Deletion Prediction with LSTM Neural Networks",
    "section": "",
    "text": "A cloze deletion test is a form of language test where a sentence (or paragraph) is given to the test taker with blanks for missing words [7]. The student is expected to fill in a “correct” word in the blanks.\nExample from Wikipedia’s article on cloze deletion [8]:\n\nToday, I went to the ____ and bought some milk and eggs.\n\nSome of the possible answers to fill in would be store, market, farm, etc.\nCloze deletion tests can be useful for language learners. These type of flashcards are described in great detail in Gabriel Wyner’s book, Fluent Forever [11]. The idea is to include a cloze deletion sentence, definition, a picture, other possibly relevant information (part of speech, conjugation, etc.). An example of these flash cards can be seen in Figure 1 on page .\n\nAfter using this method of studying for some time, I have found that certain sentences work better than other for remembering new vocabulary and grammar. Long sentences tended to be difficult to remember and were not as useful as I would tend to only look at a few words around the missing word. Cards that had a personal association were much easier to recall. Good definitions (simple and short but descriptive) helped as well.\nIn this paper I explore various machine learning approaches to predicting cloze deletion sentences from two Swedish news sources. The goals for this paper were to answer the following questions:\n\nCan we predict missing word using only the words around it?\nWhat sentences are good example sentences?\n\nDoes length of sentence make a difference?\n\nWhere are good sources to find cloze deletion sentences?\n\nI compare the difference between an LSTM (Long-Short term memory) neural network with that of a Bidirectional LSTM. Later the two news sources (described in Section 2) are compared to see which data set is easier to predict. Then I explore tuning the dropout parameter to see how overfitting can be improved. Finally the predictions are analyzed to see which sentences are easy to predict."
  },
  {
    "objectID": "posts/2018-11-05-cloze-deletion-detection/index.html#data_processing",
    "href": "posts/2018-11-05-cloze-deletion-detection/index.html#data_processing",
    "title": "Cloze Deletion Prediction with LSTM Neural Networks",
    "section": "Creating Training Examples",
    "text": "Creating Training Examples\nEach sentence is divided into potentially many training examples. For each noun, adjective, or verb in a sentence a window around the word was selected. If we let define the window \\(k\\), and a sentence is defined as \\(s = (w_0,\\ldots, w_n)\\) where \\(w_i\\) is a word in the sentence (excluding punctuation). Then for a word \\(w_i\\), we use \\((w_{i-k},\\ldots,w_{i-1},w_{i+1},\\ldots,w_{i + k})\\) to try to predict \\(w_i\\). The before window is pre-padded with zeros when there are not three words found before the target word. The after window is post-padded with zeros. For all of the experiments a window size of 3 was used to predict the words."
  },
  {
    "objectID": "posts/2018-11-05-cloze-deletion-detection/index.html#sec:model_config",
    "href": "posts/2018-11-05-cloze-deletion-detection/index.html#sec:model_config",
    "title": "Cloze Deletion Prediction with LSTM Neural Networks",
    "section": "Model Configuration",
    "text": "Model Configuration\nFor the first test, the 8 Sidor data set was used (number of sentences was \\(259,216\\).) A 30% validation set was used which resulted in \\(202,687\\) validation samples. Keras was used to implement the neural network [2]. A window size of 3 was used with \\(10,000\\) word limit on the vocabulary. Only verbs, adjectives, and nouns were used as the prediction word. Any word out of vocabulary was replaced with UNK, and if the out of vocabulary word was found in either the before window, the word, or the after window the training example was discarded. This resulted in \\(472,934\\) training examples.\n \nThe models were identical apart from the LSTM layer being bidirectional. The before window and after window are first concatenated into a single layer. Then an embedding layer is used with an embedding size of 100. The LSTM layer has 50 units, with the default Keras parameters of activation being \\(tanh\\). There is a dropout of \\(0.1\\) on both of the LSTM layers. The output layer has dimension \\(10,000 + 1\\) (the additional \\(+1\\) is to include the out of vocabulary token), with a softmax activation. The models were trained with the Adam optimizer with 30 epochs and batch size 64. The loss function is categorical cross entropy. The categorical cross entropy is a sum of each of the individual cross entropy results for each category [9]. \\[H(y, \\hat{y}) - \\frac{1}{n} \\sum_{i = 1}^{n} \\sum_{j = 1}^{m} y_{i,j} \\log(\\hat{y}_{i,j})\\] Where \\(y\\) is a vector of the true values, and \\(\\hat{y}\\) are our predictions. We define \\(n\\) as the number of examples, \\(m\\) as the number of categories and \\(y_{i,j}\\) as the \\(i\\)th example with category \\(j\\). We only have one non-zero value of \\(y_{i,j}\\) for each \\(i\\). So we can re-write this as \\[H(y, \\hat{y}) - \\frac{1}{n} \\sum_{i = 1}^{n} y_{i,c} \\log(\\hat{y}_{i,c})\\] where \\(c\\) is the only non-zero category for training example \\(i\\) since we do not have more than one category."
  },
  {
    "objectID": "posts/2018-11-05-cloze-deletion-detection/index.html#results",
    "href": "posts/2018-11-05-cloze-deletion-detection/index.html#results",
    "title": "Cloze Deletion Prediction with LSTM Neural Networks",
    "section": "Results",
    "text": "Results\n\n\n\nComparing cross-entropy loss between LSTM and Bidirectional LSTM Models\n\n\n\n\n\nComparing accuracy loss between LSTM and Bidirectional LSTM Models\n\n\nWe can see the Bidirectional LSTM model performs much better than the LSTM model on the training data. It also performs a little bit better on the validation data. From these plots we can see that both the models are over fitting but the Bidirectional LSTM model is over fitting more so than the standard LSTM model."
  },
  {
    "objectID": "posts/2018-11-05-cloze-deletion-detection/index.html#dropout-comparison",
    "href": "posts/2018-11-05-cloze-deletion-detection/index.html#dropout-comparison",
    "title": "Cloze Deletion Prediction with LSTM Neural Networks",
    "section": "Dropout Comparison",
    "text": "Dropout Comparison\n\n\n\nComparing dropout values of 0.2, 0.4, 0.6 and 0.8 on Bidirectional LSTM Model\n\n\nUsing the Bidirectional LSTM model described in the Model Config Section, an experiment was set up to see how dropout parameter affected the results on the model. All models has the Bidirectional LSTM Layer configured with the dropout set to the value \\(d = 0.2, 0.4, 0.6, 0.8\\). The models were all run with 25 epochs (\\(0,\\ldots,24\\)) that took about 300 seconds for each epoch. The recurrent dropout is also set to the same \\(d\\) value. As described in [3], the recurrent dropout randomly drops recurrent connection within the LSTM. The normal dropout parameter randomly drops the inputs and output into and out of the LSTM layer.\n\nComparing minimum loss across different dropout parameters\n\n\nDropout\nMinimum Loss\nMinimum Loss Epoch\n\n\n\n\n0.2\n3.374\n7\n\n\n0.4\n3.342\n23\n\n\n0.6\n3.451\n23\n\n\n0.8\n3.824\n24\n\n\n\nWe can see that with dropout set to \\(0.2\\), that the difference between the validation loss and the training loss is very high. The validation loss also starts to increase after epoch 7. The minimum value achieve was at epoch 7, with a categorical cross entropy of 3.374. For the other dropout values 0.4, 0.6, and 0.8 the minimum loss was 3.342, 3.451, and 3.824.\nWe can see as the dropout value increases, the training loss and validation loss are closer together, indicating less overfitting. When using a higher value of dropout, the model tends to converge slower. That is, we need more epochs to reach the same loss level. Both dropout of \\(0.6\\) and \\(0.8\\), but especially dropout of \\(0.8\\) could have been run for much longer to see where the loss converges to."
  },
  {
    "objectID": "posts/2018-11-05-cloze-deletion-detection/index.html#best-prediction-examples",
    "href": "posts/2018-11-05-cloze-deletion-detection/index.html#best-prediction-examples",
    "title": "Cloze Deletion Prediction with LSTM Neural Networks",
    "section": "Best Prediction Examples",
    "text": "Best Prediction Examples\n\nBest prediction examples for 8 Sidor data set\n\n\n\n\n\n\nWord\nSentence\n\n\n\n\ninitiativ\nPartiet Feministiskt initiativ ställer upp i valet till EUs riksdag , Europaparlamentet .\n\n\neurovision\nSanna Nielsen från Sverige sjöng i musiktävlingen Eurovision Song Contest , ESC , på tisdagen .\n\n\nchampions\nMalmö FF förlorade även den andra matchen mot Juventus i Champions League i fotboll .\n\n\nfredspris\nLiu Xiaobo från Kina får Nobels fredspris i år .\n\n\neld\nMen muslimska ledare tror att någon tänt eld på huset .\n\n\ntv\nRättegången kommer att sändas i TV 4 plus .\n\n\nbin\nUsama bin Ladin är\n\n\nförenta\nFörenta Nationernas organisation Barnfonden säger att det finns sextusen barnsoldater i Sudan i Afrika .\n\n\ngreen\nHöjdhopparen Emma Green Tregaro har också en bra chans att ta medalj .\n\n\nsos\nEmil ringde till SOS Alarm för att bli hämtad av en ambulans .\n\n\nför\nCentrum för lättläst får pengar av staten för att göra det .\n\n\nvicepresident\nDet säger USAs vicepresident Joe Biden .\n\n\nreal\nKampen står mellan Ronaldo från Real Madrid och Lionel Messi eller Andres Iniesta från Barcelona , tror experterna .\n\n\ndaglig\nDaglig verksamhet är inte ett jobb som du får lön för att göra .\n\n\nröda\nMen nu stoppar både Röda Korset och FN hjälpen till de människor som är fast i Aleppo .\n\n\nfängelse\nHan är misstänkt för spioneri och kan dömas till livstids fängelse i USA .\n\n\nprocent\nBland eleverna är Miljöpartiet tredje största parti med nästan 15 procent av rösterna .\n\n\nmeter\nSusanna Kallur vann 100 meter häck vid en gala i Karlstad på onsdagen .\n\n\nbutikskedjan\nFabian Bengtsson är chef för butikskedjan Siba som säljer elektronik .\n\n\nalarm\nFöretaget SOS Alarm har fått hård kritik den senaste tiden .\n\n\n\nWe can see the predictions from the model which had the lowest cross entropy. Many of these top predicted words are parts of proper nouns or named entities which is fairly obvious because these words don’t appear in other contexts on their own.\nSome notable example from this list that would be good cloze deletion example are: initiativ, eld, fängelse, procent, meter.\n\nPartiet Feministiskt initiativ ställer upp i valet till EUs riksdag, Europaparlamentet.\nMen muslimska ledare tror att någon tänt eld på huset. (Good in the sense that goes together frequently)\nHan är misstänkt för spioneri och kan dömas till livstids fängelse i USA.\nBland eleverna är Miljöpartiet tredje största parti med nästan 15 procent av rösterna.\nSusanna Kallur vann 100 meter häck vid en gala i Karlstad på onsdagen.\n\nFor future work, removing these named entities would potentially be better for a language learner. We can also see from these example sentences that when named entities are found within the window that the predictions are very high. For example, for predicting initiativ, the proceeding words Partiet Feministiskt are likely not seen anywhere else in the data set. These type of examples can be good for a learner that has a connection in some way to Partiet Feministiskt, to learn the word for initiativ."
  },
  {
    "objectID": "posts/2018-11-05-cloze-deletion-detection/index.html#results-1",
    "href": "posts/2018-11-05-cloze-deletion-detection/index.html#results-1",
    "title": "Cloze Deletion Prediction with LSTM Neural Networks",
    "section": "Results",
    "text": "Results\n\n\n\nComparing accuracy and cross-entropy loss between 8 Sidor and GP 2013\n\n\n\n\n\nComparing accuracy and cross-entropy loss between 8 Sidor and GP 2013\n\n\nThe training loss decreasing fast for both data sets but the model overfits the training data. The loss for the Göteborgs-Posten data set actually starts to increase with the number of epochs. Overall, the model can predict better on the 8 Sidor data set than the data from Göteborgs-Posten 2013.\nThese results are consistent with the original hypothesis. 8 Sidor’s intention is to create simple to read new articles without complicated sentence structure and words. Often readers of this newspaper are learners of the Swedish language. Göteborgs-Posten wants to be interesting to its audience, which has presumably a majority native Swedish speakers. The writers want to write in an interesting way to convey a message with a much broader vocabulary. This can be seen in the part of speech count summary table."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html",
    "title": "Inside a Basel Novartis Internship",
    "section": "",
    "text": "This summer I had the privilege of doing a 3 month summer (2019) internship at Novartis’ head office in Basel, Switzerland. I worked in the Scientific Computing and Consulting team within Biostatistics. This post is about the non-technical aspect about the internship at Novartis, particularly focused on Basel. Most of the general information applies to other sites such as Cambridge (USA), East Hanover, or other locations. This post goes into details on\n\nApplication process\nWork visa\nInternship requirements\nCampus life\nPay and benefits\nNovartis provided housing (Some warning about it as well)\n\n\n\n\nAveraged about 46 hour weeks with the max being a 57 hour week\nMonthly salary of 2500 francs plus 600 for accommodation (pre-tax)\n5 weeks vacation prorated for the length of your internship (5.5 days of vacation for my 3 months)\nFlexible working hours and potential to work from home one day a week\n\n\n\n\nBuilding designed by Frank Gehry with restaurants and a large auditorium."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#introduction",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#introduction",
    "title": "Inside a Basel Novartis Internship",
    "section": "",
    "text": "This summer I had the privilege of doing a 3 month summer (2019) internship at Novartis’ head office in Basel, Switzerland. I worked in the Scientific Computing and Consulting team within Biostatistics. This post is about the non-technical aspect about the internship at Novartis, particularly focused on Basel. Most of the general information applies to other sites such as Cambridge (USA), East Hanover, or other locations. This post goes into details on\n\nApplication process\nWork visa\nInternship requirements\nCampus life\nPay and benefits\nNovartis provided housing (Some warning about it as well)\n\n\n\n\nAveraged about 46 hour weeks with the max being a 57 hour week\nMonthly salary of 2500 francs plus 600 for accommodation (pre-tax)\n5 weeks vacation prorated for the length of your internship (5.5 days of vacation for my 3 months)\nFlexible working hours and potential to work from home one day a week\n\n\n\n\nBuilding designed by Frank Gehry with restaurants and a large auditorium."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#application-process",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#application-process",
    "title": "Inside a Basel Novartis Internship",
    "section": "Application Process",
    "text": "Application Process\nI found the job on LinkedIn with the title “Intern - Biostatistics”. The job description must be identical for all jobs in the biostatistics department since my work was nothing like it was described in the posting. I applied through a fairly standard BrassRing website in February and a few days later received an email to set up and interview. The interview was done over Skype. It was a non-technical interview where we went over my job experience. Some of the other questions were standard such as “give me an example of when you dealt with a difficult person at work”. I was interviewed by my future team, two from Cambridge and one from Basel. The job described was completely different than the job posting but I ended up accepting anyway. The whole process from application to offer was fast and professional.\nI would say that if you are interested in Pharma and are cold-applying via the website then apply to all positions that are even remotely related to what you are interested in. Even better would be to find contacts within Novartis and find a job directly. Don’t be afraid to apply without pharma experience. I only met one other intern that had worked in pharma before their internship."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#work-visa",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#work-visa",
    "title": "Inside a Basel Novartis Internship",
    "section": "Work Visa",
    "text": "Work Visa\nMy contract with Novartis was just under 90 days which meant that I did not need an official work permit. Make sure to apply early if you plan to work for over 90 days. EU citizens (particularly western-EU) have a much easier time getting a permit than non-EU countries. One benefit of staying for longer than 90 days is that you are required to register with the Basel Kanton. You then can get apply for a half-price transportation card which makes traveling a bit cheaper."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#a-global-company",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#a-global-company",
    "title": "Inside a Basel Novartis Internship",
    "section": "A Global Company",
    "text": "A Global Company\nNovartis truly feels like a global company as many teams are split across the world. Often meeting are done over Skype or similar internal tool for video conferencing. My team in particular had 3 people in Basel and 4 in Cambridge (USA). I also worked with clients in East Hanover (New Jersey). This was both a up and downside for me because it means less face-to-face interaction. I am fairly introverted and wanted to work on my interpersonal skills as much as possible. Even large meetings with 80+ people might only have 8 people in a room on-site. Many people participate in the meetings from their office. I would have liked the presentations that I gave to have had a larger in-person audience. It doesn’t feel like my public speaking skills improved that much from giving presentations over Skype. If you are uncomfortable with public speaking this is probably a plus for you!\nAn interesting feature of Basel was the language. Since it is so close to both France and Germany there are quite a few people commuting across the border. Meetings here would often start with asking which language to use. The company language was english but there was always accompanying german. My teammate are french and often would have meetings in french. Depends on your team which language you work in."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#internship-requirements",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#internship-requirements",
    "title": "Inside a Basel Novartis Internship",
    "section": "Internship Requirements",
    "text": "Internship Requirements\nThere are not a lot of requirements for the Basel Novartis internship. Many interns have to give a end of internship presentation. I believe this was set up automatically in Cambridge and is a requirement. In Basel it seems that your supervisor has to set things up themselves. I ended up giving one bigger presentation and many to clients of my projects. It depends a lot on the field you are in as well.\nThere is also a two day welcome-day for all new employees which you should make sure to ask your supervisor about. I was unaware of it and was not registered for it. It seemed like a good way for new employees to meet each other and explore the opportunities around campus.\nYou will have to do some virtual training via the up4Growth internal platform. I had to do general safety training and well as ethics, and proper reporting of data. There are additional in-person trainings you will need to complete if you are working in a lab."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#hiring-interns",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#hiring-interns",
    "title": "Inside a Basel Novartis Internship",
    "section": "Hiring Interns",
    "text": "Hiring Interns\nNovartis has a large number of interns and post-docs. There were many Master’s students I met doing their thesis work at Novartis. The biostatistics department had many PhD students as well. Other interns were post-graduate and had already completed their degree. I did not meet anyone that was doing their Bachelors degree. At least at Basel, the hiring rate for interns seemed pretty low. This is just compared with my experience in the US. Many of the new hires in my department were senior level employees.\nMost of the interns at Novartis have longer contracts and are expected to work for at least 3 months. I would say that the majority of the interns that I met were doing 6 month internships. The demands of each group vary dramatically. Some interns had really light work loads on mundane projects and others had high workloads on important projects.\nIf your goal is to work at Novartis after finishing your internship know that you have to let your team know your goals. If you don’t tell them you want to work for Novartis you will never get hired. For me this was not an issue as I was not looking for a full-time job from Novartis after my internship."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#campus-life",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#campus-life",
    "title": "Inside a Basel Novartis Internship",
    "section": "Campus Life",
    "text": "Campus Life\nLife at Novartis campus is pretty awesome. They have a virtual tour available and if you are in Basel you can get an art tour of the campus. The architecture of the new buildings is really beautiful. There is a building designed by Frank Gehry. An overview of the other architecture can be found here. One of my favorite buildings was Asklepios, a new building overlooking the Rhine. The view from inside the building is fantastic. You can even take a swim in the Rhine during lunch.\n\n\n\nThe view from Asklepios of the Rhine\n\n\nThe campus is like a small city. You have a coop supermarket, many restaurants, coffee places (even starbucks), ice cream vendors, apple trees, koi pond, and lounge chairs. There is lots of trees and grass to have an enjoyable lunch. Coffee, tea and water are provided in the building. The free coffee is mediocre (the kaffee créme beans are better than the espresso roast) but you can always have a better cup of coffee at one of the cafés around campus.\n\n\n\nNice areas on campus to take a break\n\n\nThere are always interesting talks and presentations going on around campus. Biostatistics in particularly always seemed to have technical presentations going on. There is lots of effort put into professional development via presentations and courses. I attended a “biostatistics boot camp” which was one of my favorite parts of the summer. I highly recommend it to anyone interested in how drugs progress in human trials, from Phase 1 to market and beyond."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#pay-and-benefits",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#pay-and-benefits",
    "title": "Inside a Basel Novartis Internship",
    "section": "Pay and benefits",
    "text": "Pay and benefits\nThe base salary for a Master’s student is 2500 francs per month. There is also a living stipend of 600 francs per month. They provide one-way transportation fees up to 400 francs. Your salary is paid into a Novartis bank account where you can transfer money to EU banks. Interns receive vacation prorated for the amount of time they are staying based on the 5-week full time employee yearly vacation. For me this meant I had 5.5 days vacation. Depending on your contract you probably need to clock in your hours. This is done with your badge when you come into your building or go out for lunch. If you miss a clock-in there is an ancient web interface to do it on as well.\nNovartis has flexible working hours, which means that you are supposed to have within 25 hours of the monthly full time hours (40 hours per week). This means that if you work more than 8 hours in a day you accrue flextime, up to 25 hours. If you need to leave for a half-day you can take your flextime without any issue. I ended up accumulating lots of hours of flextime and was able to take three full days off in the middle of my internship to get a full week off due to a Thursday/Friday holiday for the Swiss National Day. I ended my internship with a huge surplus of flextime which I was told will be paid out. I did not end up taking my 5.5 days of vacation which will be paid out as well.\nIn general most people that I worked with were extremely busy and worked many hours. The older, more established employees seemed to take vacation regularly and it was common to have auto-reply emails for all of August. My building was a ghost town in August. Your work-life balance depends a lot on the group you are in. On average I worked about 46 hour weeks with the max being 57. I worked about two half days on the weekend but this was my choice to be better prepared for the week.\nYou are also allowed to work 20 percent from home (one full day a week). I didn’t really take advantage of this since I wanted to interact with as many people as I could."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#accomodations",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#accomodations",
    "title": "Inside a Basel Novartis Internship",
    "section": "Accomodations",
    "text": "Accomodations\nNovartis provides an options for housing through Aprentas housing. I stayed at the Eglisee Wohnheim Aprentas house which cost 700 francs per month for a “small” room. There are also larger rooms for 800 available. It is about 20–30 minutes by tram and about 35 minutes to walk. Biking is really fast if you have one. I ended up walking most days when it was not raining.\nIt was nice that I was able to move to Basel without having to worry about find a place to live. The house had capacity for 30 people, 15 guys and 15 girls. Almost everyone was an intern for Novartis which makes it easy to meet people. There was a shared kitchen and everyone had cleaning duty for a week at a time. Overall everyone was really nice and the staff took good care of the place. There was a large proportion of Italians in the house that seemed to always be cooking some amazing smelling food.\n\nWarning: Move in on the first or the sixteenth of the month\nOne gotcha with this was the first months rent (and potentially the last months as well). The housing provided charged full price if you moved in the first 15 days of the month and half price any of the second half days. Okay, seems reasonable. Novartis pro-rates the reimbursement amount based on exactly the number of days you worked this month. So if you move in on the first or 16, no problem. So if you move in the 15th, you will receive ~300 from Novartis but pay the full 700 amount. Similarly if you move in right before the end of the month you will be charged 300 and only receive a small stipend. I told Novartis HR about this but no policy change has occurred so make sure to confirm with them if you get an internship.\n\n\nWarning: Aprentas housing in Muttenz\nThere is a larger Aprentas housing in Muttenz. It is almost 45 minutes away from Novartis by tram and you are far out of the city center. For most people this housing option is probably worse. The benefits are it is really close to a bouldering gym, B2 Boulders and Bar as well as closer to the alps and popular tourist cities (Interlaken, Grindelwald, Bern, etc.)"
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#closing-thoughts",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#closing-thoughts",
    "title": "Inside a Basel Novartis Internship",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nOverall I had an extremely positive internship experience with Novartis. I would highly recommend anyone interested in pharma to apply. It is a fast-paced environment that will challenge you and make you grow. I made so many good connections this summer and will definitely consider working their after I do my PhD. Feel free to reach out for any questions.\nIf you enjoyed this then stay tuned for my follow up on life in Basel, and the technical portions of my internship."
  },
  {
    "objectID": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#resources",
    "href": "posts/2019-09-01-inside-a-basel-novartis-internship/index.html#resources",
    "title": "Inside a Basel Novartis Internship",
    "section": "Resources",
    "text": "Resources\n\nNovartis Website\nNovartis LinkedIn\nNovartis Open Source\nAprentas Housing\nLife in Basel\nWork permits in Switzerland\nQuora: How can I win an internship at Roche or Novartis in Basel?"
  },
  {
    "objectID": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html",
    "href": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html",
    "title": "Sum of uniform random variables until sum is greater than one",
    "section": "",
    "text": "I saw this problem on r/probabilitytheory\n\nIf you pick a uniformly random real number on [0,1] and repeat this until the sum of numbers picked is greater than 1, you’ll on average pick \\(e \\approx 2.718\\) numbers\n\nPosed in a way that we can actually begin to solve this.\n\nSample independent uniform random variables \\(U_1, U_2, \\ldots\\) and let \\(S_n = \\sum_{i = 1}^n U_i\\). Let \\(N\\) be the first integer \\(n\\) such that \\(S_n &gt; 1\\). Find \\(E[N]\\)\n\nBonus question: What is the \\(E[S_n]\\)? Can we find its distribution?\nOf course, the first step is to replicate the simple simulation."
  },
  {
    "objectID": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html#introduction",
    "href": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html#introduction",
    "title": "Sum of uniform random variables until sum is greater than one",
    "section": "",
    "text": "I saw this problem on r/probabilitytheory\n\nIf you pick a uniformly random real number on [0,1] and repeat this until the sum of numbers picked is greater than 1, you’ll on average pick \\(e \\approx 2.718\\) numbers\n\nPosed in a way that we can actually begin to solve this.\n\nSample independent uniform random variables \\(U_1, U_2, \\ldots\\) and let \\(S_n = \\sum_{i = 1}^n U_i\\). Let \\(N\\) be the first integer \\(n\\) such that \\(S_n &gt; 1\\). Find \\(E[N]\\)\n\nBonus question: What is the \\(E[S_n]\\)? Can we find its distribution?\nOf course, the first step is to replicate the simple simulation."
  },
  {
    "objectID": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html#simulation",
    "href": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html#simulation",
    "title": "Sum of uniform random variables until sum is greater than one",
    "section": "Simulation",
    "text": "Simulation\n\n# Sample from independent uniform U(0,1) distributions and stop when the sum\n# is greater than 1. Let N be that first sum. Then E[N] = e\n\nsim_sum &lt;- function(t = 1) {\n  s_n &lt;- 0\n  n &lt;- 0\n  while(s_n &lt; 1) {\n    s_n &lt;- s_n + runif(1)\n    n &lt;- n + 1\n  }\n  c(n = n, s_n = s_n)\n}\n\n\nreps &lt;- 1e4\nN_sim &lt;- data.frame(t(replicate(reps, sim_sum())))\nN_sim_table &lt;- table(N_sim$n)\nN_sim_prob &lt;- N_sim_table / reps\n\nK &lt;- as.numeric(names(N_sim_table))\n\nmean_N &lt;- mean(N_sim$n)\nplot(K, N_sim_prob, type = 'b', ylim = c(0, 1), ylab = 'P(N = n)', xlab = 'N')\nabline(v = mean_N, lty = 2)\ntext(x = mean_N + 1, y = 0.75, sprintf('Mean(n) = %.03f', mean_N))"
  },
  {
    "objectID": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html#proof",
    "href": "posts/2022-11-10-sum-of-uniform-random-variables-relation-to-e/index.html#proof",
    "title": "Sum of uniform random variables until sum is greater than one",
    "section": "Proof",
    "text": "Proof\nThe idea of the proof is to directly solve:\n\\[\nE[n] = \\sum_{n = 1}^\\infty n \\cdot P(N = n)\n\\]\nSo we need to calculate, \\(P(N = n)\\). This is the same as, \\(P(S_{n - 1} \\leq 1 \\text{ and } S_n &gt; 1)\\). If we calculate \\(P(S_n \\leq x) = F_{S_n}(x)\\) we can complete the proof.\n\n\\(P(S_n \\leq x)\\)\nUsing the proof from this answer on math.stackexchange,\nwhen \\(x \\in [0,1]\\), claim is that \\(P(S_n \\leq t) = \\frac{x^n}{n!}\\). When \\(n = 1\\), then \\(P(S_1 \\leq t) = P(U_1 \\leq t) = t\\). Assume true for \\(n\\). Then,\n\\[\n\\begin{aligned}\nP(S_{n + 1} \\leq x) &= P(S_n + U_{n + 1} \\leq t)\\\\\n&= \\int_0^1 P(S_n + u \\leq t) \\underbrace{f(u)}_{1} ~du && S_n \\text{ and } U_{n + 1} \\text{ independent}\\\\\n&=  \\int_0^1 P(S_n \\leq t - u) ~du\\\\\n&=  \\int_0^t \\underbrace{P(S_n \\leq t - u)}_{\\text{Induction Hypothesis}} ~du && \\text{Since } P(S_n \\leq t - u) = 0 \\text{ when } u \\in [t, 1]\\\\\n&= \\int_0^t \\frac{(t - u)^n}{n!} ~du \\\\\n&= \\frac{1}{n!} \\left( \\frac{-1}{n + 1} (t - u)^{n + 1}\\Big|^{u = t}_{u = 0} \\right)\\\\\n&= \\frac{t^{n + 1}}{(n + 1)!}\n\\end{aligned}\n\\]\n\n\nPMF of \\(S_n\\)\nSince \\(F_{S_n}(x) = \\frac{x^n}{n!}\\) we have \\[\n\\begin{aligned}\nf_{S_n}(x) &= \\frac{dF_{S_n}}{dx}\\\\\n&= n \\frac{x^{n - 1}}{n!}\n\\end{aligned}\n\\]\n\n\n\\(P(N = n)\\)\n\\[\n\\begin{aligned}\nP(N = n) &= P(S_{n - 1} \\leq 1 \\text{ and } S_n &gt; 1)\\\\\n&= P(S_n &gt; 1 | S_{n - 1} \\leq 1) P(S_{n - 1} \\leq 1)\\\\\n&= \\int_0^1 P(U_n + x &gt; 1 | S_{n - 1} = x \\leq 1) \\underbrace{P(s \\leq 1 | S_{n - 1} = s)}_{1} f_{S_{n - 1}}(x)~dx\\\\\n&= \\int_0^1 P(U_n + x &gt; 1)~f_{S_{n - 1}}(x)~dx && U_n \\text{ and } S_{n - 1} \\text{ independent}\\\\\n&= \\int_0^1 (1 - F_n(1 - x))~f_{S_{n - 1}}(x)~dx\\\\\n&= \\int_0^1 x (n - 1)\\frac{x^{n - 2}}{(n - 1)!}~dx\\\\\n&= \\frac{n - 1}{(n - 1)!} \\int_0^1 x^{n - 1}~dx\\\\\n&= \\frac{n - 1}{n (n - 1)!} = \\frac{n - 1}{n!}\n\\end{aligned}\n\\]\nTo double check the answer we can simulate\n\n# This is formula we computed analytically\n# (K - 1) / (factorial(K))\nK &lt;- as.numeric(names(N_sim_table))\nexpected_probs &lt;- (K - 1) / (factorial(K))\n\nplot(K, N_sim_prob, type = 'b', xlab = 'n', ylab = 'Prob(N = n)', ylim = c(0, 1), yaxp = c(0, 1, 4))\nlines(K, expected_probs, type = 'b', col = 'red')\n\n\n\n\n\n\nE[N] = e\n\\[\n\\begin{aligned}\nE[N] &= \\sum_{n = 2}^\\infty n \\cdot P(N = n)\\\\\n&=  \\sum_{n = 2}^\\infty \\frac{n (n - 1)}{n!}\\\\\n&= \\sum_{n = 2}^\\infty \\frac{1}{(n - 2)!}\\\\\n&=  \\sum_{n = 0}^\\infty \\frac{1}{n!} && \\text{since } \\sum_{n = 0}^\\infty \\frac{x^n}{n!} = e^x\\\\\n&= e\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "",
    "text": "We are considering a dataset that provides some demographic information for 440 of the most populous counties in the United States in years 1990-92. Each line of the dataset provides information on 14 variables for a single county. Counties with missing data were deleted from the dataset. We are building models to predict the number of crimes per 1000 people in each county. We first explore linear regression models and then a negative binomial regression model. We found that the best linear regression model performed similarly to the negative binomial regression model on the training and test set using the same variables."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#introduction",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#introduction",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "",
    "text": "We are considering a dataset that provides some demographic information for 440 of the most populous counties in the United States in years 1990-92. Each line of the dataset provides information on 14 variables for a single county. Counties with missing data were deleted from the dataset. We are building models to predict the number of crimes per 1000 people in each county. We first explore linear regression models and then a negative binomial regression model. We found that the best linear regression model performed similarly to the negative binomial regression model on the training and test set using the same variables."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#goals",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#goals",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Goals",
    "text": "Goals\nThe goals of the analysis was the find a model that is simple yet explains as much as possible. The model should make sense first and foremost. Automatic methods such as the backwards step algorithm were used as auxiliary methods to supplement a more hand selected model. We decided against using all possible subsets selection as a matter of principle as it can lead to models that lack explainability. We explore interactions between variables as well as standard additive models. Once model selection is done based on the training set the final results are reported against the test set (20% of the dataset). The test set was not looked at or used in the model building process. The models were compared on the training set using 10-fold cross validation and leave one out cross validation (LOOCV)."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#data-processing",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#data-processing",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Data Processing",
    "text": "Data Processing\nSome additional variables were created. The variables “beds”, “phys”, and “area” were all divided by the population to give a per capita total number of hospital beds, cribs and bassinets, per capita number of physicians practicing, and the per capita area. We then remove the total quantities from the data set. We found that working with per capita was more informative than the direct quantities. The data was also transformed almost entirely with natural log transform as it performed better for our model. We arrived at natural log based on the plots, residuals plots, and looking how the model’s \\(R^2\\) changed with regard to the transformations along with cross validation. The data summary can be found in the appendix."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#models",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#models",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Models",
    "text": "Models\n\nFull Model\ncrm1000 ~ percapitaarea + popul + pop1834 + pop65plus +\n          percapitaphys + percapitabeds + higrads + bachelors +\n          poors + unemployed + percapitaincome + region\nThe full model was used as a baseline model, and subsequent models were reduced from this model.\n\nBase Model\ncrm1000 ~ percapitaarea + popul + pop1834 +\n          percapitabeds + poors + percapitaincome + region\nThese are the variables from which all the other models are based. We arrived at this model by keeping one variable from each correlation cluster that seemed to make the most intuitive sense to keep. We confirmed that this was a good model by starting with all of the variables and using backward selection to arrive at the identicial model that we had selected by hand. Then a partial F-test was performed, see table below. The F statistic was extremely small, which means that the residual sum of squares was almost the same after dropping the variables pop65plus, percapitaphys, higrads, bachelors, and unemployed. From this reduced model, we built up variations that involved transformations and interactions.\n\n\n\nPartial F-Test for Full model vs final model (without transformations)\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n342\n143881.3\nNA\nNA\nNA\nNA\n\n\n337\n143017.6\n5\n863.651\n0.407\n0.844\n\n\n\n\n\n\n\n\n\n\n(Final Model) Log Transformed model with no interactions\nThis model ended up being our final model selection. The transformation were found through cross validation and analysis of the plots of the data.\ncrm1000 ~ log(percapitaarea) + log(popul) + log(pop1834) +\n          log(percapitabeds) + log(poors) + log(percapitaincome) + region\n\n\nTransformations and Interactions with only region.\nWe included all interactions between the continuous variables and the region. We then backward selected that reduced to (log) population and (log) per capita income against the region. While running the backwards selection we checked that the main effects were not dropped while the interactions were kept.\ncrm1000 ~ log(percapitaarea) + log(popul) + log(pop1834) +\n          log(percapitabeds) + log(poors) + log(percapitaincome) +\n          region + log(popul):region + log(percapitaincome):region\n\n\nTransformations and Interactions against all variables.\nWe included the interactions between all variables (including continuous-continuous) then backward selected. Both of the interactions models included log(popul):region but the larger interaction model did not include log(percapitaincome):region. We suspect this may be from the correlation between per capita income and some of the other variables that are included such as the percentage of poor people.\ncrm1000 ~ log(percapitaarea) + log(popul) + log(pop1834) +\n          log(percapitabeds) + log(poors) + log(percapitaincome) +\n          region + log(popul):log(percapitabeds) +\n          log(popul):log(poors) + log(popul):log(percapitaincome) +\n          log(popul):region + log(pop1834):region +\n          log(percapitabeds):region + log(poors):log(percapitaincome) +\n          log(poors):region\n\n\nNegative Binomial Generalized Linear Model\nWe used the same variables from the log transformed model with no interactions. Instead of modeling crime rate per 1000 people directly we model the crimes. We use an offset to adjust the parameters according to the population (more is described in Negative Binomial Model section).\ncrimes ~ offset(log(popul / 1000)) + log(percapitaarea) +\n         log(popul) + log(pop1834) + log(percapitabeds) +\n         log(poors) + log(percapitaincome) + region\n\n\nOther models\nFor our cross-validation and model selection we included two models which we did not intend to use but only for comparision purposes. They were the full model which was described above. Also, we included a model which we call the simple model which only included the natural log of the percentage of poor people in the county."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#multicollinearity",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#multicollinearity",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nIn this dataset there are many variables that are highly correlated (see appendix for correlation plot). We found that the percentage of poor people in a county was highly correlated with the per capita income and the number of high school grads. If we include all of the variables with high multicollinearity the standard errors will be increased and some of the variables may not end up being significant. We used the Variance inflation factor (VIF) to measure the multicollinearity in our model. The VIF gives us a measure of how much the coefficient’s variance will increase due to collinearity between the other variables [2]. The VIF factors are the computed by regressing all the other explanatory variables on a single variable \\(x_i\\). For each variable \\(i\\), we calculate the VIF by performing \\(i\\) regressions using all other variables to predict \\(x_i\\). \\[\nx_i = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_{i - 1} x_{i - 1} + \\beta_{i + 1} x_{i + 1} + \\cdots \\beta_{p-1} x_{p-1}\n\\] Then the VIF factor is equal to \\[\nVIF_i = \\frac{1}{1 - R_i^2}\n\\] Where \\(R_i^2\\) is the \\(R^2\\) when using all other variables in a regression on variable \\(i\\). [4] We removed the variables for per capita income, the number of high school graduates, and the number of bachelor degree graduates based on the VIF results. The last column in the table shows how much the standard error of the variable is increased due to the collinearity. For example, for the bachelors variable the value of 2.834 means that the standard error for \\(\\beta_{bachelors}\\) is 2.834 larger than if the variables were not correlated with each other.\n\n\n\n\n\n\nVIF\nDF\nVIF^(1/(2*Df))\n\n\n\n\nbachelors\n7.716\n1\n2.778\n\n\npercapitaincome\n5.490\n1\n2.343\n\n\nhigrads\n5.022\n1\n2.241\n\n\npoors\n4.461\n1\n2.112\n\n\npercapitabeds\n3.582\n1\n1.893\n\n\npercapitaphys\n3.435\n1\n1.853\n\n\nregion\n2.716\n3\n1.181\n\n\npop1834\n2.320\n1\n1.523\n\n\nunemployed\n2.304\n1\n1.518\n\n\npop65plus\n2.146\n1\n1.465\n\n\npercapitaarea\n1.563\n1\n1.250\n\n\npopul\n1.245\n1\n1.116"
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#outliers",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#outliers",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Outliers",
    "text": "Outliers\nWhen looking at the training data we noticed one outlier with respect to the target variable in the training dataset (Kings County in New York). It has a crime rate of approximately 296 per 1000 people. The median for the training set was a crime rate of 53.55 per 1000 people. In the training set we found that Kings County has larger leverage and also has high influence. There are also outliers with respect to the other variables but the amount is reduced when we took the log transform of the data. In the appendix, we go into more detailed look at other outliers based on the studentized residuals and influence."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#interactions",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#interactions",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Interactions",
    "text": "Interactions\nWe explored and built models that included interactions between the variables. We found that while some of the interactions were significant, when doing cross validation the additive model without any interactions performed the best. Using the partial F-test, we compared the two interaction models with the additive model. For the interactions model with only region, we did not reject the null hypothesis at the \\(\\alpha = 0.05\\) level, as the p-value was 0.06. The value was close enough that we decided to continue to compare with this model as well. The results can be see in the table below. When we run a partial F-test on the interaction models with more interactions, we get a larger test statistic that allows us to reject the null hypothesis at the \\(\\alpha = 0.05\\) level.\n\n\n\nPartial F-test for region interactions model\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n342\n121929.0\nNA\nNA\nNA\nNA\n\n\n336\n117312.6\n6\n4616.4\n2.204\n0.042\n\n\n\n\n\n\n\n\n\n\nPartial F-test for full interactions model\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n342\n121929.0\nNA\nNA\nNA\nNA\n\n\n326\n106585.4\n16\n15343.55\n2.933093\n0.00015\n\n\n\n\n\n\n\nIn the conditional plot we can see the values of the log of population vs the crime rate for each of the regions. The bottom left panel is the West, bottom right is the northeast, top left is the midwest and top right is the south. There seems to be a slightly smaller slope in the West panel, but overall it is not clear that the interactions are needed.\n\n\n\n\n\nOur original cross validation was performed on data that was not centered (mean subtracted off). Centering can improve the standard errors and thus the p-values of the estimates but the predictions for the new values are the same. Since our final model does not include the interactions, we decided against centering the data. When building the interaction models we included all of the interactions (for region and then for all variables) and then backward selected. We did this twice: once with the data centered and once without the data being centered. While this does not make a difference for the estimate, the standard errors decrease in the centered model. That made the backwards selection algorithm stop earlier when we centered the data, and thus included more of the interaction variables. Again, at this point we checked to make sure that no interaction effects were included when a main effect was dropped. When including these centered models in our cross validation we found that they performed worse than the backward selection models on the original (non-standardized) data so we chose not to include them in our analysis."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#model-interpretation",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#model-interpretation",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Model Interpretation",
    "text": "Model Interpretation\nOn the training data, we had an \\(R^2\\) of 0.56 which means that 54.3% of the error is explained by our model. The \\(R_{adj}^2\\) was 0.548.\n\n\n\n\n\n\nEstimate\nStd. Error\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-360.441\n105.325\n0.001\n\n\nlog(percapitaarea)\n-5.346\n1.462\n0.000\n\n\nlog(popul)\n6.490\n1.944\n0.001\n\n\nlog(pop1834)\n19.330\n7.858\n0.014\n\n\nlog(percapitabeds)\n4.351\n2.495\n0.082\n\n\nlog(poors)\n25.918\n3.844\n0.000\n\n\nlog(percapitaincome)\n22.224\n9.501\n0.020\n\n\nregionnortheast\n-17.144\n3.863\n0.000\n\n\nregionmidwest\n-9.936\n3.791\n0.009\n\n\nregionsouth\n4.436\n3.425\n0.196\n\n\n\n\n\n\n\nLet’s look at what these estimates actually mean. Since we are working on a log scale we can interpret the estimate \\(\\beta_{area}\\) for log(percapitaarea) as the change in crime rate per 1000, \\(crm_{1000}\\), when \\(log(percapitaarea)\\) increases by 1. That is, \\(\\ln x_{area} + 1 = \\ln(e \\cdot x_{area})\\). So if \\(x_{area}\\) is multiplied by \\(e \\approx 2.718\\), then \\(crm_{1000}\\) increases by \\(\\beta_{area}\\). It can be easier to interpret if we look at percentage increase instead of multiplying by \\(e\\). If the per captia area increases by 10%, then the crime rate per 1000 people will decrease by 0.49. \\(\\beta_{area} \\cdot ln(1.10) = -5.127 \\cdot ln(1.10) = -0.49\\). This is summarized in the table\n\n\n\nChange in crime rate by percentage increase\n\n\n\n5%\n10%\n20%\n30%\n\n\n\n\npercapitaarea\n-0.261\n-0.510\n-0.975\n-1.403\n\n\npopul\n0.317\n0.619\n1.183\n1.703\n\n\npop1834\n0.943\n1.842\n3.524\n5.072\n\n\npercapitabeds\n0.212\n0.415\n0.793\n1.142\n\n\npoors\n1.265\n2.470\n4.725\n6.800\n\n\npercapitaincome\n1.084\n2.118\n4.052\n5.831\n\n\n\n\n\n\n\nFrom this table we can see that if the the percentage of poor people increased from 20 to 22, we would expect to see an increase of 2.428 in the number of crimes per 1000 people. We can also see that if the per captia area increases from \\(5 \\times 10^{-3}\\) to \\(5.5 \\times 10^{-3}\\) we would expect the number of crimes per 1000 people to drop by 0.489.\n\nRegion\nWe used “West” as our reference category so all the parameter estimates are in relation to the west region. We can interpret the estimate \\(\\beta_{region_{NE}} = -18.548\\) as the estimate change in crimes per 1000 people between the west and the north east. That is, holding all else constant, the north-east region is estimated to have \\(18.548\\) less crimes per 1000 people. Both the north-east and the midwest are statistically significant at the \\(\\alpha = 0.05\\) level. The difference between the south and the west’s rate of crimes is not statistically significant. That means that the there is not enough evidence from the data to show there is a difference in crime rates between the south and the west (assuming all else is held constant)."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#negative-binomial-model",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#negative-binomial-model",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Negative Binomial Model",
    "text": "Negative Binomial Model\nWe also explored generalized linear models for predicting the crime rate. Since we are dealing with the rate \\(1000 * crimes/popul\\), we can formulate our model as\n\\[\nlog(crimes) = log(popul/1000) + X\\beta\n\\]\nSince we are using the (log) population in our model, having an offset is equivalent to the value of the coefficient for log(population) will increased by 1, and the intercept decreased by log(1000). We first tested a Poisson regression which assumed that the variance and expected value are the same for the crimes. When performing a dispersion test at \\(\\alpha = 0.05\\), we rejected the null that dispersion is equal to one. (Estimated dispersion was 2622.163). When dispersion is greater than one, we say that the data is overdispersed. The negative binomial model works better in the presence of overdispersion.\n\nInterpreting Results\n\n\n\n\n\n\nEstimate\np-value\n\n\n\n\n(Intercept)\n-3.262\n0.057\n\n\nlog(percapitaarea)\n-0.065\n0.006\n\n\nlog(popul)\n0.116\n0.000\n\n\nlog(pop1834)\n0.330\n0.010\n\n\nlog(percapitabeds)\n0.105\n0.010\n\n\nlog(poors)\n0.417\n0.000\n\n\nlog(percapitaincome)\n0.428\n0.006\n\n\nregionnortheast\n-0.404\n0.000\n\n\nregionmidwest\n-0.219\n0.000\n\n\nregionsouth\n0.045\n0.416\n\n\n\n\n\n\n\nWe can interpret the results from negative binomial similarly to Poisson regression [1] [3]. Let’s look at how the percentage of poor people in a county influences the number of crimes. We have that \\[\n\\log(crimes) = \\beta_0 + \\beta_{poors} \\log x_{poors} + \\cdots\n\\] or \\[\ncrimes = \\exp(\\beta_0 + \\beta_{poors} \\log x_{poors}  + \\cdots) = e^{\\beta_0} x_{poors}^{\\beta_{poors}} \\cdots\n\\] Now if we want to see what happens as \\(x_{poors}\\) changes, say to \\(x_{poors}^{\\prime}\\), then we have that \\[\n\\frac{crimes^{\\prime}}{crimes} = \\left(\\frac{x_{poors}^{\\prime}}{x_{poors}}\\right)^{\\beta_{poors}}\n\\] That is, the percentage change in the crimes is equal to the percentage change in percentage of poor people to the power of the coefficient for poors. Equivalently we pose this additively as \\[\n\\log crimes^{\\prime} - \\log crimes = \\beta_{poors} \\left( \\log x_{poors}^{\\prime} - \\log x_{poors} \\right)\n\\] So if we kept all other variables constant, and increased \\(x_{poors}\\) from 10 to 12 we would expect: \\[\n\\frac{crimes^{\\prime}}{crimes} = \\left(\\frac{12}{10} \\right)^{0.414} = 1.078\n\\] That is, we would expect the crimes to increase by 7.8%. We can confirm this for the predicted crimes per 1000 people while holding all other variables at the means. When \\(x_{poors} = 10\\), then the predicted crime rate is 72.18. The predicted crime from for \\(x_{poors} = 12\\) is 77.88. The ratio of these values is 1.079 which is what we found from the coefficient interpretation.\nIt is also useful to look at how the predicts change as one of the variables varies. We set all of the variables involved in the regression to the means and then only varied the percentage of poor from 1 to 37, which is approximately the range found in the training set. It is easier to see how the crime rate changes from this graph due to the poor percentage changing than just the estimated coefficients."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#cross-validation",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#cross-validation",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Cross Validation",
    "text": "Cross Validation\nOnce we had built our models we used 10-fold cross validation to compare them against each other. We also used LOOCV which resulted in very similar results to the 10-fold cross validation. Once we had included the negative binomial model we switched to only using the 10-fold cross validation. The interactions models had a better adjusted \\(R^2\\) value and were found with the partial F-test to have at least one parameters that should not be set to 0 (at alpha = 0.05 significance level). We included a model called “Full”, which had all of the dependent variables included (without transformation) and a “simple” model which is only using the natural log of the percentage of poor people for each county.\n\n\n\nAverage MSE across 10-folds\n\n\nAdditive\nNB\nRegionInter\nAllInter\nFull\nSimple\n\n\n\n\n376.1\n391.38\n409.8\n421.52\n489.51\n606.04"
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#test-validation",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#test-validation",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Test Validation",
    "text": "Test Validation\nAt this point of the analysis everything had been conducted on the training 80% of the dataset. Cross validation was used as part of the model building process which means that the results will be biased. We kept the test set held out until the end when all models were finalized so that the estimated mean squared error is a better indication of the true mean squared error. The results on the test sets are as follows\n\n\n\nTest MSE will all data from test included\n\n\nAllInter\nNB\nAdditive\nRegionInter\nFull\nSimple\n\n\n\n\n245.68\n251.68\n262.09\n276.91\n300.24\n459.16\n\n\n\n\n\n\n\n\n\n\nTest MSE with Kings County excluded\n\n\nAllInter\nAdditive\nNB\nRegionInter\nFull\nSimple\n\n\n\n\n231.13\n246.12\n248.63\n252.25\n296.38\n452.95\n\n\n\n\n\n\n\nWe can see that including Kings County (NY) leads to slightly better results in the test set. Based on these results, we can see that both the negative binomial and the additive model without interactions performs the best and almost identically. Since linear regression is faster and simpler to interpret the results our final model is the Log Transformed model with no interactions, in the models section.\nUsing this as our final model, we can fit this model to the test selection data. This can be seen as a more conservative approach to our parameters since it is on the test set rather than the training.\n\n\n\nTest set coefficients and p-values\n\n\n\nEstimate\nStd. Error\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-616.416\n199.524\n0.003\n\n\nlog(percapitaarea)\n-1.448\n2.794\n0.606\n\n\nlog(popul)\n4.967\n3.525\n0.163\n\n\nlog(pop1834)\n19.648\n11.078\n0.080\n\n\nlog(percapitabeds)\n3.228\n4.428\n0.468\n\n\nlog(poors)\n25.484\n6.820\n0.000\n\n\nlog(percapitaincome)\n51.636\n18.351\n0.006\n\n\nregionnortheast\n-23.065\n6.468\n0.001\n\n\nregionmidwest\n-5.330\n7.119\n0.456\n\n\nregionsouth\n8.088\n6.582\n0.223\n\n\n\n\n\n\n\nWe have \\(R^2_{adj} =\\) 0.552 and \\(R^2 =\\) 0.598. Which means that 59.8 percent of the error is explained by our model. The p-values in this table are the most conservative, since we have not seen any of the data when testing this hypothesis. In this way, it is like a properly run experiment. We do not want to fish for statistically significance by creating hypotheses based on our data set. Instead if we formulate our hypothesis beforehand we should use the data set to validate our hypothesis.\n\nFinal Coefficients\nCombining the training and test data sets and fitting our final model on this gives us our final coefficients. We want to use all the data if we are going to predict for more counties in the future. We can interpret these estimates exactly the same as for the training and test data. The p-values are artificially low, due to a large sample size and the fact that 80% of the data is from the training set which we used to generated the hypothesis.\n\n\n\nFinal coefficients and p-values\n\n\n\nEstimate\nStd. Error\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-387.885\n92.606\n0.000\n\n\nlog(percapitaarea)\n-5.009\n1.276\n0.000\n\n\nlog(popul)\n6.108\n1.697\n0.000\n\n\nlog(pop1834)\n18.891\n6.415\n0.003\n\n\nlog(percapitabeds)\n4.327\n2.163\n0.046\n\n\nlog(poors)\n25.606\n3.345\n0.000\n\n\nlog(percapitaincome)\n25.910\n8.400\n0.002\n\n\nregionnortheast\n-18.795\n3.311\n0.000\n\n\nregionmidwest\n-9.823\n3.303\n0.003\n\n\nregionsouth\n4.531\n3.009\n0.133\n\n\n\n\n\n\n\nWe find it easier to understand some of these estimates with plots. Below we show how the predicted crime rates change when holding all variables constant (keeping the other variables at the mean) while varying only the population. We range the population from the minimum 100043 to the maximum 8863164."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#conclusion",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#conclusion",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Conclusion",
    "text": "Conclusion\nIn this analysis we investigated the crime rates for 440 counties in the United States. The dataset was split into 80% training data and 20% test data. The test dataset was untouched until all models were finalized. We compared multiple linear regression models and a negative binomial model and compared the results. The models were compared using 10-fold cross validation as part of the model building process as well as for validation. The most predictive variables in the data set were the population, the percentage of the population between 18 and 34, the per captia beds, the percentage of poor people, the per capita income, and the region in the United States. We showed that the difference between West and Northeast crime rate had a statistically significant difference and well as the difference between West and Midwest."
  },
  {
    "objectID": "posts/2019-02-25-census-crime-rate-prediction/index.html#appendix",
    "href": "posts/2019-02-25-census-crime-rate-prediction/index.html#appendix",
    "title": "1990 to 1992 Census Crime Rate Prediction.",
    "section": "Appendix",
    "text": "Appendix\n\nData Summary\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nidentification number, 1–440.\n\n\ncounty\ncounty name.\n\n\nstate\nstate abbreviation.\n\n\narea\nland area (square miles).\n\n\npopul\nestimated 1990 population.\n\n\npop1834\npercent of 1990 CDI population aged 18–34.\n\n\npop65plus\npercent of 1990 CDI population aged 65 years old or older.\n\n\nphys\nnumber of professionally active nonfederal physicians during 1990.\n\n\nbeds\ntotal number of hospital beds, cribs and bassinets during 1990.\n\n\ncrimes\ntotal number of serious crimes in 1990 (including murder, rape, robbery, aggravated assault, burglary, larceny-theft, motor vehicle theft).\n\n\nhigrads\npercent of adults (25 yrs old or older) who completed at least 12 years of school.\n\n\nbachelors\npercent of adults (25 yrs old or older) with bachelor’s degree.\n\n\npoors\nPercent of 1990 CDI population with income below poverty level.\n\n\nunemployed\npercent of 1990 CDI labor force which is unemployed.\n\n\npercapitaincome\nper capita income of 1990 CDI population (dollars).\n\n\ntotalincome\ntotal personal income of 1990 CDI population (in millions of dollars).\n\n\nregion\nGeographic region classification used by the U.S. Bureau of the Census, where 1=Northeast, 2 = Midwest, 3=South, 4=West.1\n\n\npercapitabeds\nnumber of beds (see descriptions above) per capita\n\n\npercapitaarea\narea per capita\n\n\npercaptiaphys\nphysicians per capita\n\n\n\n\n\nModel Diagnostics\n\nStudentized Residuals\n\n\n\n\n\n\n\n\ncounty\nstate\nStudent Residuals\n\n\n\n\nKings\nNY\n12.753\n\n\nAtlantic\nNJ\n3.490\n\n\nSt._Louis_City\nMO\n3.183\n\n\nEctor\nTX\n2.817\n\n\nLeon\nFL\n2.570\n\n\nDelaware\nIN\n2.480\n\n\nJefferson\nKY\n2.369\n\n\nMadison\nAL\n2.341\n\n\nPulaski\nAR\n2.097\n\n\nColumbiana\nOH\n2.088\n\n\nShawnee\nKS\n2.012\n\n\nClay\nMO\n1.981\n\n\n\n\n\n\n\nWe can see from the Studentized residuals that there are a few notable points. Kings county in NY is extremely far away from other points. We compared models with and without Kings county but we found that keeping Kings county in the data set had a better test mean squared error.\n\n\n\nDFBETA on continuous coefficients\n\n\n\n\n\n\n\n\nHigh &gt; 2/sqrt(n) DFBETA values\n\n\ncounty\ncoefficient\nDFBETA\n\n\n\n\nKings (NY)\nlog(percapitaincome)\n-5.579\n\n\nKings (NY)\nlog(pop1834)\n-5.005\n\n\nAtlantic (NJ)\nlog(percapitaincome)\n3.532\n\n\nKings (NY)\nlog(poors)\n3.401\n\n\nKings (NY)\nlog(percapitaarea)\n-2.924\n\n\nLeon (FL)\nlog(pop1834)\n2.610\n\n\nKings (NY)\nlog(percapitabeds)\n-2.463\n\n\nFulton (GA)\nlog(percapitaincome)\n2.322\n\n\nEctor (TX)\nlog(percapitaincome)\n1.700\n\n\nWestchester (NY)\nlog(percapitaincome)\n-1.634\n\n\nPitt (NC)\nlog(pop1834)\n-1.578\n\n\nClay (MO)\nlog(percapitaincome)\n-1.499\n\n\nColumbiana (OH)\nlog(percapitaincome)\n1.410\n\n\nDelaware (IN)\nlog(pop1834)\n-1.399\n\n\nManatee (FL)\nlog(pop1834)\n-1.234\n\n\nPitt (NC)\nlog(percapitaincome)\n-1.222\n\n\nSarasota (FL)\nlog(pop1834)\n-1.208\n\n\nPhiladelphia (PA)\nlog(percapitaincome)\n1.169\n\n\nColumbiana (OH)\nlog(pop1834)\n1.144\n\n\nAlachua (FL)\nlog(pop1834)\n1.045\n\n\nAtlantic (NJ)\nlog(poors)\n1.033\n\n\nDavis (UT)\nlog(percapitaincome)\n1.002\n\n\nCollier (FL)\nlog(percapitaincome)\n0.994\n\n\nYolo (CA)\nlog(percapitaincome)\n0.990\n\n\nHampshire (MA)\nlog(pop1834)\n-0.990\n\n\n\n\n\n\n\nThis table shows the sorted extreme \\(DFBETA\\) values for each of the coefficients. Take for example Monroe (IN), which has a huge influence on the \\(pop1834\\) parameter. If we look at the data, we see that Monroe has 26.1, 29% of its population between 18 and 34. We see that Kings (NY), influences many of the coefficients and has some of the most extreme values. It is by far the most influential and extreme point in the data.\n\nModel Diagnostic Plots\n\n\n\n\n\nWe can see from the diagnostic plots that the residuals and fitted values are approximately symmetric and the residuals don’t appear to be increasing or decreasing as the fitted values increases. The normal QQ plot shows that we have larger tails than we would like. The estimates for the parameters do not dependent on it following a normal distribution but our p-values should be interpreted a little pessimistically. The two points that are labeled, 6 (Kings) and 1 (Los Angeles) are outliers. We can see that Los Angeles has a high leverage and also an extreme standardized residual which makes it influential. Kings county is closer to the mean for the dependent variables so it’s leverage is lower. It is still marked by the Residuals vs Leverage plot as an issue as its residual is so high.\n\n\n\nCorrelations\n\n\n\n\n\nThis clustered distance matrix was how we selected which variables to remove from the model. The blue variables are negatively correlated with each other and the red variables are positively correlated.\n\n\nOutliers\n\n\n\n\n\nWe can see that Kings county is high for its region and overall."
  },
  {
    "objectID": "posts/2020-05-17-joint-distribution-of-sums-of-exponential-random-variables/index.html",
    "href": "posts/2020-05-17-joint-distribution-of-sums-of-exponential-random-variables/index.html",
    "title": "Joint distribution of sums of exponential random variables",
    "section": "",
    "text": "This is a problem from Ross’s Stochastic Processes [1]. Let \\[\n\\begin{aligned}\nS_1 &= X_1\\\\\nS_2 &= X_1 + X_2\\\\\nS_3 &= X_1 + X_2 + X_3\n\\end{aligned}\n\\] where \\(X_1, X_2, X_3\\) are i.i.d exponential random variables with rate \\(\\lambda\\). Find the joint distribution of \\(S_1, S_2, S_3\\).\nLet \\(f\\) be the PDF of each \\(X_1, X_2, X_3\\) (since identically distributed). Since \\(X_1, X_2, X_3\\) are independent the joint PDF is \\[\nf(x,y,z) = f(x) f(y) f(z) = \\lambda^3 e^{-\\lambda x} e^{-\\lambda y} e^{-\\lambda z}\n\\] Then we can find the joint CDF of \\(S_1, S_2, S_3\\) \\[\n\\begin{aligned}\nP(S_1 \\leq t_1, S_2 \\leq t_2, S_3 \\leq t_3) &= \\int_{0}^{t_1} \\int_{0}^{t_2 - x} \\int_{0}^{t_3 - x - y} f(x,y,z) ~dz~dy~dx\\\\\n&= \\int_{0}^{t_1} \\int_{0}^{t_2 - x} \\int_{0}^{t_3 - x - y} \\lambda e^{-\\lambda z} \\lambda e^{-\\lambda y} \\lambda e^{-\\lambda x}~dz~dy~dx\\\\\n&= \\int_{0}^{t_1} \\lambda e^{-\\lambda x} \\int_{0}^{t_2 - x} (1 - e^{-\\lambda (t_3 - x - y)}) \\lambda e^{-\\lambda y}~dy~dx\\\\\n&= \\int_{0}^{t_1} \\lambda e^{-\\lambda x} \\left[\\int_{0}^{t_2 - x} \\lambda e^{-\\lambda y}~dy -  \\int_{0}^{t_2 - x} e^{-\\lambda (t_3 - x)}~dy\\right]~dx\\\\\n&= \\int_{0}^{t_1} \\lambda e^{-\\lambda x} \\left[(1 - e^{-\\lambda (t_2 - x)}) - (t_2 - x) e^{-\\lambda (t_3 - x)}\\right]~dx\\\\\n&= \\int_{0}^{t_1} \\lambda e^{-\\lambda x} ~dx - \\int_{0}^{t_1} \\lambda e^{-\\lambda t_2} ~dx - \\int_{0}^{t_1} (t_2 - x) e^{-\\lambda t_3}~dx\\\\\n&= 1 - e^{-\\lambda t_1} - \\lambda t_1 e^{-\\lambda t_2} - t_1 t_2 e^{-\\lambda t_3} + \\frac{1}{2} t_1^2 e^{-\\lambda t_3}\n\\end{aligned}\n\\]\n\nSimulation\nWe can confirm these results with a simple simulation in R.\n\n# Computed joint CDF\nexpect_joint &lt;- function(t1, t2, t3, lambda = 1) {\n  1 - exp(- lambda * t1) - lambda * t1 * exp(-lambda * t2) - \n    t1 * t2 * exp(-lambda * t3) + 1/2 * t1^2 *exp(- lambda * t3)\n}\n\n# Simulate 1000 realizations of S1, S2, S3\nsim &lt;- function(t1, t2, t3, rate = 1, n = 1000) {\n  s1 &lt;- rexp(n, rate)\n  s2 &lt;- s1 + rexp(n, rate)\n  s3 &lt;- s2 + rexp(n, rate)\n  \n  mean(s1 &lt;= t1 & s2 &lt;= t2 & s3 &lt;= t3)\n}\n\n# P(S1 &lt;= 1, S2 &lt;= 2, S3 &lt;= 3) with lambda = 1\nt1 &lt;- 1\nt2 &lt;- 2\nt3 &lt;- 3\nrate &lt;- 1\n# Replicate the simulation 1000 times\nsim_res &lt;- replicate(1000, sim(t1, t2, t3, rate))\n\ncat(\"Simulated mean:\", round(mean(sim_res), 3))\n\nSimulated mean: 0.422\n\ncat(\"Expected joint distribution:\", round(expect_joint(t1, t2, t3, rate), 3))\n\nExpected joint distribution: 0.422\n\n\n```\n\n\n\n\n\n\n\n\nReferences\n\n[1] Ross, S.M. et al. 1996. Stochastic processes. Wiley New York."
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html",
    "title": "Introduction to Time Series",
    "section": "",
    "text": "\\[\nX_t = m_t + s_t + y_y\n\\] where - \\(m : \\mathbb{Z} \\rightarrow \\mathbb{R}\\) is a slowly changing function, the trend component. - \\(s : \\mathbb Z \\rightarrow \\mathbb R\\) is a function with a known period \\(d\\), i.e., \\(s_{t + d} = s_t\\) for all \\(t \\in \\mathbb Z\\) and \\(\\sum_{j = 1}^d s_j = 0\\), is called the seasonal component. - \\((y_t, t \\in \\mathbb Z)\\) is a stationary stochastic process."
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#classical-decomposition-model",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#classical-decomposition-model",
    "title": "Introduction to Time Series",
    "section": "",
    "text": "\\[\nX_t = m_t + s_t + y_y\n\\] where - \\(m : \\mathbb{Z} \\rightarrow \\mathbb{R}\\) is a slowly changing function, the trend component. - \\(s : \\mathbb Z \\rightarrow \\mathbb R\\) is a function with a known period \\(d\\), i.e., \\(s_{t + d} = s_t\\) for all \\(t \\in \\mathbb Z\\) and \\(\\sum_{j = 1}^d s_j = 0\\), is called the seasonal component. - \\((y_t, t \\in \\mathbb Z)\\) is a stationary stochastic process."
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#time-series-analysis",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#time-series-analysis",
    "title": "Introduction to Time Series",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\n\nAlways plot the data first\n\nIf there are clear sections in the data, it might be good to analyze each section separately"
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#best-linear-predictor",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#best-linear-predictor",
    "title": "Introduction to Time Series",
    "section": "Best Linear Predictor",
    "text": "Best Linear Predictor\nLet \\((X_t, t \\in \\Z)\\) be a time series with \\(Var(X_t) &lt; \\infty\\) for \\(t \\in \\Z\\) and \\(X^n := (X_{t_1},\\ldots, X_{t_n})\\) be a collection of random variables of the time series at \\(n\\) different times. Then the _best linear predictor of \\(X_t\\) is given by \\[\nb_t^l(X^n) = a_0 + a_1 X_{t_n} + \\cdots + a_n X_{t_1}\n\\] where the coefficients are determined by the linear equations\n\n\\(E(X_t - b_t^l(X^n)) = 0\\)\n\\(E(X_{t_j}(X_t - b_t^l(X^n))) = 0\\) for all \\(j = 1,\\ldots,n\\).\n\nIf X is stationary, with mean \\(\\mu\\) and autocovariance function \\(\\gamma\\), the coefficients are determined by \\[\na_0 = \\mu (1 - \\sum_{i = 1}^n a_i)\n\\] and \\[\n(\\gamma(t_{n + 1 - j} - t_{n + 1 - i}))_{i,j = 1}^n (a_1,\\ldots,a_n)^T = (\\gamma(t - t_n),\\ldots, \\gamma(t - t_1))^T\n\\]\nThe mean square error is:\n\\[\nMSE(b_t^l(X^n), X_t) = E[(b_t^l(X^n) - X_t)^2] = \\gamma(0) - (a_1, \\ldots, a_n)(\\gamma(t - t_n), \\ldots, \\gamma(t - t_1))^T\n\\]\nNote: when \\(X^n := (X_1, \\ldots, X_n)\\) then the coefficients \\((a_0,\\ldots,a_n)\\) for prediction of \\(X_{n + h}\\) and \\[\n(\\gamma(i - j)_{i,j = 1}^n) (a_1,\\ldots, a_n)^T = (\\gamma(h), \\ldots, \\gamma(h + n - 1))^T\n\\]\n\nExample\nThe notation \\(t_1,\\ldots, t_n\\) was confusing to me at first. A good example that shown how it works is if we have an \\(AR(1)\\) process, \\[\nX_t - \\phi_1 X_{t - 1} = Z_t\n\\] Assume that we observe values at \\(X_1\\) and \\(X_3\\), but are missing a value for \\(X_2\\). We then have \\(t_1 = 1\\), \\(t_2 = 3\\), with \\(n = 2\\). We want to find: \\[\nb_2^l(X^1,X^3) = a_0 + a_1 X_3 + a_2 X_1\n\\] We have that \\(a_0 = 0\\), since the process has mean zero. It then follows that,\n\\[\n\\begin{aligned}\n  \\begin{pmatrix}\n    \\gamma(t_{3 + 1 - 1} - t_{3 + 1 - 1}) & \\gamma(t_{3 + 1 - 2} - t_{3 + 1 - 1})\\\\\n     \\gamma(t_{3 + 1 - 1} - t_{3 + 1 - 2}) &\\gamma(t_{3 + 1 - 1} - t_{3 + 1 - 1})\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    a_1\\\\\n    a_2\n  \\end{pmatrix}\n  &=\n  \\begin{pmatrix}\n    \\gamma(2 - t_{2})\\\\\n    \\gamma(2 - t_{1})\n  \\end{pmatrix}\\\\\n  \\begin{pmatrix}\n    \\gamma(0) & \\gamma(3 - 1)\\\\\n     \\gamma(3 - 1) &\\gamma(0)\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    a_1\\\\\n    a_2\n  \\end{pmatrix}\n  &=\n  \\begin{pmatrix}\n    \\gamma(2 - 3)\\\\\n    \\gamma(2 - 1)\n  \\end{pmatrix}\\\\\n  \\begin{pmatrix}\n    1 & \\phi_1^2\\\\\n     \\phi_1^2 & 1\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    a_1\\\\\n    a_2\n  \\end{pmatrix}\n  &=\n  \\begin{pmatrix}\n    \\phi_1\\\\\n    \\phi_1\n  \\end{pmatrix}\n\\end{aligned}\n\\]\nWhich leads to a solution \\[\na_1 = a_2 = \\frac{\\phi_1}{1 + \\phi_1^2}\n\\]"
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#armapq",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#armapq",
    "title": "Introduction to Time Series",
    "section": "ARMA(p,q)",
    "text": "ARMA(p,q)\n\\(\\{X_t\\}\\) is an ARMA(p,q) process if \\(\\{X_t\\}\\) is stationary and if for every \\(t\\), \\[\nX_t - \\sum_{i = 1}^p \\phi_i X_{t-i} = Z_t + \\sum_{j = 1}^q \\theta_j Z_{t - j}\n\\] where \\(\\{Z_t\\} \\sim WN(0, \\sigma^2)\\) and the polynomials \\((1 - \\sum_{i = 1}^p \\phi_i z^i)\\) and \\((1 + \\sum_{j = 1}^q \\theta_j z^j)\\) have no common factors."
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#causality",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#causality",
    "title": "Introduction to Time Series",
    "section": "Causality",
    "text": "Causality\nAn ARMA(p,q) process \\(\\{X_t\\}\\) is causal, if there exists constants \\(\\{\\psi_t\\}\\) such that \\[\n\\sum_{j = 0}^\\infty | \\psi_{j} | &lt; \\infty\n\\] and \\[\nX_t = \\sum_{j = 0}^\\infty \\psi_{j} Z_{t - j}\n\\] for all \\(t\\). That is, if we can represent the ARMA(p,q) process \\(\\{X_t\\}\\) as a \\(MA(\\infty)\\) process.\nHow to actually check? Use the equivalent condition: \\[\n\\phi(z) = 1 - \\sum_{i = 1}^p \\phi_i z^i \\not = 0 \\quad \\text{for all}~|z| \\leq 1\n\\] That is, that \\(\\phi(z)\\) has no roots inside (or that all root are outside the unit circle).\n\nFinding Causal Representation\nTo represent our (causal) ARMA(p,q) process in the form: \\[\n  X_t = \\sum_{j = 0}^\\infty \\psi_j Z_{t - j}\n\\] The sequence of \\(\\{\\psi_j\\}\\) is determined by the relation \\(\\psi(z) = \\sum_{j = 0}^\\infty \\psi_j z^j = \\theta(z) / \\phi(z)\\), or equivalently: \\[\n\\psi_j - \\sum_{k = 1}^p \\phi_k \\psi_{j - k} = \\theta_j, ~j = 0,1,\\ldots\n\\] with \\(\\theta_0 := 1, \\theta_j := 0\\) for j &gt; q and \\(\\psi_j := 0\\) for \\(j &lt; 0\\).\n\n\nFinding invertible (AR(\\(\\infty\\))) representation\nTo represent our invertible ARMA(p,q) process in the form: \\[\nZ_t = \\sum_{j = 0}^\\infty \\pi_j X_{t - j}\n\\] we find \\(\\{\\pi_j\\}\\) by the equations \\[\n\\pi_j + \\sum_{k = 1}^q \\theta_k \\pi_{j - k} = -\\phi_j, ~j = 0,1,\\ldots,\n\\] where \\(\\phi_0 := -1,~ \\phi_j := 0\\) for \\(j &gt; p\\), and \\(\\pi_j := 0\\) for \\(j &lt; 0\\)."
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#model-building-for-arma-processes",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#model-building-for-arma-processes",
    "title": "Introduction to Time Series",
    "section": "Model Building for ARMA processes",
    "text": "Model Building for ARMA processes\n\nRemove trend and seasonality until you believe the dta can be modeled as a stationary time series.\nIdentify the order of the ARMA process for the time series\n\nEither look at ACF/PACF\nor by fitting (using maximum likelihood or Hannan-Rissanen estimation) sucessively higher order ARMA(p,q) to the data and choosing p, q to minimize either the AICC or BIC.\n\nEstimate the final model using maximum likelihood\nCompute the residuals \\(\\hat{R}_t\\) and check that they are consistent with the specified distribution and temporal covariance structure for \\(Z_t\\).\n\nIf they are, then the model is considered adequate for the data."
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#autoregressive-processes---arp",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#autoregressive-processes---arp",
    "title": "Introduction to Time Series",
    "section": "Autoregressive Processes - AR(p)",
    "text": "Autoregressive Processes - AR(p)\n\\[\nX_t - \\sum_{j = 1}^p \\phi_j X_{t - j} = Z_t\n\\]\n\nAR(p) is not necessarily stationary!\n\n\nAR(1) Example\n\n\n\n\n\n\n\nAR(2)\nCan simulate using the function astsa::arima.sim function \\[\nX_t = Z_t + 0.7 \\cdot X_{t - 1} + 0.2 \\cdot X_{t - 2}\n\\]"
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#moving-average-processes---maq",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#moving-average-processes---maq",
    "title": "Introduction to Time Series",
    "section": "Moving Average Processes - MA(q)",
    "text": "Moving Average Processes - MA(q)\n\\[\nX_t = Z_t + \\sum_{j = 1}^q \\theta_j Z_{t - j}\n\\] with \\(Z_t \\sim WN(0, \\sigma^2)\\). At lag greater than or equal to q, the ACF should be zero.\n\nMA(2) example\n\\[\nX_t = Z_t + 0.8 \\cdot Z_{t - 1} + 0.2 \\cdot Z_{t - 2}\n\\]\n\n\n\n\n\nThe autocorrelation function"
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#armapq-1",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#armapq-1",
    "title": "Introduction to Time Series",
    "section": "ARMA(p,q)",
    "text": "ARMA(p,q)\n\nThe partial autocorrelation function PACF of an ARMA(p,q) process X can be thought of as the correlation between \\(X_t\\) and \\(X_{t + h}\\) when adjusting for the intervening observations \\(X_{t + 1}, \\ldots, X_{t + h - 1}\\).\n\nCan be used to detect seasonality: If you have monthly data and you see a big spike at \\(\\hat{\\alpha}(12)\\), it is likely that you see a periodic effect corresponding to a calendar year, so you should remove the seasonality.\n\n\n\nACF and PACF\n\n\n\n\n\n\n\n\nConditional Mean Model\nACF\nPACF\n\n\n\n\nAR(p)\nTails off gradually\nCuts off after p lags\n\n\nMA(q)\nCuts off after q lags\nTails off gradually\n\n\nARMA(p,q)\nTails off gradually\nTails off gradually\n\n\n\nSource: https://se.mathworks.com/help/econ/autocorrelation-and-partial-autocorrelation.html"
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#innovations-algorithm",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#innovations-algorithm",
    "title": "Introduction to Time Series",
    "section": "Innovations Algorithm",
    "text": "Innovations Algorithm\nUsed to compute the best linear predictor, \\(b_{n + 1}^l(X^n)\\) more computationally efficiently than solving a system of \\(n\\) linear equations. Can be applied to all time series with finite second moments. Assume we have a time series \\((X_t, t \\in \\mathbb Z)\\) with zero mean, and finite second moment \\(E[X_t^2] &lt; + \\infty\\) for all \\(t \\in \\mathbb Z\\) and covariance \\[\nCov(X_i, X_j) = \\kappa(i,j)\n\\] We denote the best linear one-step predictors as \\[\n\\hat{X}_n := \\begin{cases}\n  0 & \\text{for n = 1}\\\\\n  b_{n}^l(X^{n - 1}) & \\text{for n &gt; 1}\n\\end{cases}\n\\] and the mean squared errors as \\[\nv_n := MSE(\\hat{X}_{n+1}, X_{n + 1}) = E[(\\hat{X}_{n + 1} - X_{n + 1})^2]\n\\]\nThere exists coefficients \\((\\theta_{ij}, 1 \\leq j \\leq i \\leq n)\\) such that the best linear predictors satisfy \\[\n\\hat{X}_{n + 1} = \\begin{cases}\n  0 & \\text{for } n = 0\\\\\n  \\sum_{j = 1}^n \\theta_{nj}(X_{n + 1 - j} - \\hat{X}_{n+1-j}) & \\text{for } n \\geq 1\n\\end{cases}\n\\] We compute the coefficients \\(\\theta_{n1},\\ldots, \\theta_{nn}\\) recursively from the equations \\[\nv_0 := \\kappa(1,1)\n\\] and \\[\n\\theta_{n (n - k)} := v_k^{-1} \\left(\n  \\kappa(n + 1, k + 1) - \\sum_{j = 0}^{k - 1} \\theta_{k (k - j)} \\theta_{n (n - j)} v_j\n\\right)\n\\]"
  },
  {
    "objectID": "posts/2019-07-06-introduction-to-time-series/index.html#garcharch",
    "href": "posts/2019-07-06-introduction-to-time-series/index.html#garcharch",
    "title": "Introduction to Time Series",
    "section": "GARCH/ARCH",
    "text": "GARCH/ARCH\n\nConditional Maximum Likelihood\nThe MLEs \\((\\hat{\\alpha}_0, \\ldots, \\hat{\\alpha}_p, \\hat{\\beta}_1, \\ldots, \\hat{\\alpha}_q, \\hat{\\theta}_Z)\\) are obtained by maximizing the conditional likelihood function \\[\nL(\\alpha_0, \\ldots, \\alpha_p, \\beta_1, \\ldots, \\alpha_q, \\theta_Z) = \\prod_{t = p + 1} \\frac{1}{\\sigma_t} f_Z \\left( \\frac{x_t}{\\sigma_t}\\right)\n\\] where \\(f_Z\\) is the density of the white noise Z and \\(\\theta_Z\\) is any other parameter Z depends on (such as degrees of freedom if Z is t-distributed).\nIt \\(Z \\sim IIDN(0,1)\\) then, \\[\n\\begin{aligned}\nf_Z \\left( \\frac{x_t}{\\sigma_t}\\right) &= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( - \\frac{X_t^2}{2 \\sigma_t^2}\\right)\\\\\nL(\\alpha_0, \\ldots, \\alpha_p, \\beta_1, \\ldots, \\alpha_q, \\theta_Z) &= \\prod_{t = p + 1}^n \\frac{1}{\\sigma_t} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( - \\frac{X_t^2}{2 \\sigma_t^2}\\right)\\\\\n- \\ln L(\\alpha_0, \\ldots, \\alpha_p, \\beta_1, \\ldots, \\alpha_q, \\theta_Z) &= \\sum_{t = p + 1}^n \\ln{\\sigma_t} + \\frac{1}{2} \\ln(2\\pi)as + \\frac{1}{2} \\frac{X_t^2}{\\sigma_t^2}\\\\\n&= \\frac{1}{2} \\sum_{t = p + 1}^n 2 \\ln{\\sigma_t} + \\ln(2\\pi) + \\frac{X_t^2}{\\sigma_t^2}\\\\\n&= \\frac{1}{2} \\sum_{t = p + 1}^n \\ln{\\sigma_t^2} + \\ln(2\\pi) + \\frac{X_t^2}{\\sigma_t^2}\n\\end{aligned}\n\\]\n\n\nAcknowledgements\nThis blog post was made possible thanks to:\n\n(Xie, Hill, and Thomas, 2017)\n(Boettiger, 2021)\n(Wickham, Chang, Flight, Müller et al., 2021)\n\n\n\nReferences\n\n[1] C. Boettiger. knitcitations: Citations for ‘Knitr’ Markdown Files. R package version 1.0.12. 2021. URL: https://CRAN.R-project.org/package=knitcitations.\n\n\n[2] H. Wickham, W. Chang, R. Flight, K. Müller, et al. sessioninfo: R Session Information. R package version 1.2.2. 2021. URL: https://CRAN.R-project.org/package=sessioninfo.\n\n\n[3] Y. Xie, A. P. Hill, and A. Thomas. blogdown: Creating Websites with R Markdown. ISBN 978-0815363729. Boca Raton, Florida: Chapman and Hall/CRC, 2017. URL: https://bookdown.org/yihui/blogdown/.\n\n\n\nReproducibility\n\n\n─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Catalina 10.15.7\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2022-12-11\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package       * version date (UTC) lib source\n P assertthat      0.2.1   2019-03-21 [?] CRAN (R 4.2.0)\n P backports       1.4.1   2021-12-13 [?] CRAN (R 4.2.0)\n P bibtex          0.5.0   2022-09-25 [?] CRAN (R 4.2.0)\n P cli             3.4.1   2022-09-23 [?] CRAN (R 4.2.0)\n P colorspace      2.0-3   2022-02-21 [?] CRAN (R 4.2.0)\n P DBI             1.1.3   2022-06-18 [?] CRAN (R 4.2.0)\n P digest          0.6.31  2022-12-11 [?] CRAN (R 4.2.1)\n P dplyr           1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P evaluate        0.18    2022-11-07 [?] CRAN (R 4.2.0)\n P fansi           1.0.3   2022-03-24 [?] CRAN (R 4.2.0)\n P fastmap         1.1.0   2021-01-25 [?] CRAN (R 4.2.0)\n P generics        0.1.3   2022-07-05 [?] CRAN (R 4.2.0)\n P ggplot2       * 3.4.0   2022-11-04 [?] CRAN (R 4.2.0)\n P glue            1.6.2   2022-02-24 [?] CRAN (R 4.2.0)\n P gtable          0.3.1   2022-09-01 [?] CRAN (R 4.2.0)\n P htmltools       0.5.4   2022-12-07 [?] CRAN (R 4.2.0)\n P htmlwidgets     1.5.4   2021-09-08 [?] CRAN (R 4.2.0)\n P httr            1.4.4   2022-08-17 [?] CRAN (R 4.2.1)\n P jsonlite        1.8.4   2022-12-06 [?] CRAN (R 4.2.1)\n P knitcitations * 1.0.12  2021-01-10 [?] CRAN (R 4.2.0)\n P knitr           1.41    2022-11-18 [?] CRAN (R 4.2.1)\n P lifecycle       1.0.3   2022-10-07 [?] CRAN (R 4.2.0)\n P lubridate       1.9.0   2022-11-06 [?] CRAN (R 4.2.0)\n P magrittr        2.0.3   2022-03-30 [?] CRAN (R 4.2.0)\n P munsell         0.5.0   2018-06-12 [?] CRAN (R 4.2.0)\n P pillar          1.8.1   2022-08-19 [?] CRAN (R 4.2.0)\n P pkgconfig       2.0.3   2019-09-22 [?] CRAN (R 4.2.0)\n P plyr            1.8.8   2022-11-11 [?] CRAN (R 4.2.0)\n P R6              2.5.1   2021-08-19 [?] CRAN (R 4.2.0)\n P Rcpp            1.0.9   2022-07-08 [?] CRAN (R 4.2.0)\n P RefManageR      1.4.0   2022-09-30 [?] CRAN (R 4.2.0)\n   renv            0.16.0  2022-09-29 [1] CRAN (R 4.2.0)\n P rlang           1.0.6   2022-09-24 [?] CRAN (R 4.2.0)\n P rmarkdown       2.18    2022-11-09 [?] CRAN (R 4.2.0)\n P rstudioapi      0.14    2022-08-22 [?] CRAN (R 4.2.0)\n P scales          1.2.1   2022-08-20 [?] CRAN (R 4.2.0)\n P sessioninfo   * 1.2.2   2021-12-06 [?] CRAN (R 4.2.0)\n P stringi         1.7.8   2022-07-11 [?] CRAN (R 4.2.0)\n P stringr         1.5.0   2022-12-02 [?] CRAN (R 4.2.0)\n P tibble          3.1.8   2022-07-22 [?] CRAN (R 4.2.0)\n P tidyselect      1.2.0   2022-10-10 [?] CRAN (R 4.2.0)\n P timechange      0.1.1   2022-11-04 [?] CRAN (R 4.2.0)\n P utf8            1.2.2   2021-07-24 [?] CRAN (R 4.2.0)\n P vctrs           0.5.1   2022-11-16 [?] CRAN (R 4.2.0)\n P withr           2.5.0   2022-03-03 [?] CRAN (R 4.2.0)\n P xfun            0.35    2022-11-16 [?] CRAN (R 4.2.0)\n P xml2            1.3.3   2021-11-30 [?] CRAN (R 4.2.0)\n P yaml            2.3.6   2022-10-18 [?] CRAN (R 4.2.0)\n\n [1] /Users/stefaneng/personal_devel/stefanengineering.comV3/renv/library/R-4.2/x86_64-apple-darwin17.0\n [2] /Users/stefaneng/personal_devel/stefanengineering.comV3/renv/sandbox/R-4.2/x86_64-apple-darwin17.0/84ba8b13\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2020-05-15-probability-of-even-using-probability-generating-functions/index.html",
    "href": "posts/2020-05-15-probability-of-even-using-probability-generating-functions/index.html",
    "title": "Probability of even/odd using probability generating functions",
    "section": "",
    "text": "A probability generating function for a discrete random variable \\(X\\) taking values \\(\\{0,1,\\ldots\\}\\) is defined as \\[\nG(z) = E[z^X] = \\sum_{j = 0}^\\infty z^j P(X = j)\n\\]\nWhich is defined for all complex \\(z\\) in which the sum converges. It always converges for \\(|z| &lt; 1\\) but the radius might be larger depending on our distribution. Clearly we have that \\(G(0) = 0\\). Also we can see that \\[\nG(1) = \\sum_{j = 0}^\\infty P(X = j) = 1\n\\] since we are summing over the entire sample space.\n\n\nThis is problem 1.11 in Ross’s Stochastic Processes Second Edition.\nWe have that \\[\n\\begin{aligned}\nG(-1) &= \\sum_{j = 0}^\\infty (-1)^j P(X = j)\\\\\n  &= P(X = 0) - P(X = 1) + P(X = 2) - \\cdots\\\\\nG(1) &= P(X = 0) + P(X = 1) + P(X = 2) + \\cdots\\\\\nG(-1) + G(1) &= 2P(X = 0) + 2P(X = 2) + 2 P(X = 4) + \\cdots\n\\end{aligned}\n\\]\nAssuming that 0 is considered even it follows that \\[\n\\begin{aligned}\nP(X \\text{ is even}) = \\frac{G(1) + G(-1)}{2} = \\frac{1 + G(-1)}{2}\n\\end{aligned}\n\\] We also have that \\[\nP(X \\text{ is odd}) = 1 - \\frac{1 + G(-1)}{2} = \\frac{1 - G(-1)}{2}\n\\]\n\n\nAssume that \\(X\\) is a binomial with parameters \\(n\\) and \\(p\\) \\[\nP(X = x) = {n \\choose x} p^x (1 - p)^{n - x}\n\\] Then we can compute the probability generating function\n\\[\n\\begin{aligned}\nG(z) = E[z^X] &= \\sum_{j = 0}^\\infty z^j P(X = j)\\\\\n  &= \\sum_{j = 0}^\\infty z^j {n \\choose j} p^j (1 - p)^{n - j}\\\\\n  &= \\sum_{j = 0}^n {n \\choose j} (zp)^j (1 - p)^{n - j}\\\\\n  &= (zp + 1 - p)^n && \\text{Binomial theorem}\n\\end{aligned}\n\\]\n\npgf_binom &lt;- function(z, n, p) {\n  (z * p + 1 - p)^n\n}\n\nz &lt;- seq(0, 1, length.out = 1000)\nn &lt;- 10\np &lt;- 1/3\nplot(z, pgf_binom(z, n, 3/4), type = \"l\")\n\nNote that the sum converges for all real \\(z\\). This is a good point to check that our result matches that \\(G(1) = 1\\). So \\(G(-1) = (1 - 2p)^n\\). Thus, \\[\nP(X \\text{ is even}) = \\frac{1 + (1 - 2p)^n}{2}\n\\]\n\n\n\nLet \\(X\\) be a Poisson random variable with mean \\(\\lambda\\) \\[\nP(X = k) = e^{-\\lambda} \\frac{\\lambda^k}{k!}\n\\]\nThen \\[\n\\begin{aligned}\nG(z) = E[z^X] &= \\sum_{j = 0}^\\infty z^j e^{-\\lambda} \\frac{\\lambda^j}{j!}\\\\\n  &= e^{-\\lambda} \\sum_{j = 0}^\\infty  \\frac{(\\lambda z)^j}{j!}\\\\\n  &= e^{-\\lambda} e^{\\lambda z}\\\\\n  &= e^{-\\lambda + \\lambda z}\n\\end{aligned}\n\\] Note that the sum converges for all real \\(z\\). Again as an exercise check that \\(G(1) = 1\\). It follows that \\[\nG(-1) = e^{-2\\lambda}\n\\] So the probability of a Poisson random variable being even is \\[\nP(X \\text{ is even}) = \\frac{1 + e^{-2\\lambda}}{2}\n\\]\n\n\n\nAssume now that \\(X\\) is geometric with parameter \\(p\\) with \\(X \\in \\{1,2,\\ldots\\}\\). \\[\nP(X = k) = p (1 - p)^{k - 1}\n\\] Then the probability generating function for \\(X\\) is\n\\[\n\\begin{aligned}\nG(z) = E[z^X] &= \\sum_{j = 1}^\\infty z^j P(X = j)\\\\\n  &= \\sum_{j = 1}^\\infty z^j p (1 - p)^{j - 1}\\\\\n  &= pz \\sum_{j = 1}^\\infty (z(1 - p))^{j - 1}\\\\\n  &= pz \\sum_{j = 0}^\\infty (z(1 - p))^{j}\\\\\n  &= \\frac{pz}{1 - z(1 - p)} && \\text{ for } |z| &lt; \\frac{1}{1 - p}\n\\end{aligned}\n\\] Since \\(1 - p &lt; 1\\), then the sum converges for \\(z = -1\\). It follows that \\[\nG(-1) = \\frac{- p}{2 - p}\n\\] So the probability that the geometric random variable \\(X\\) is even is given by\n\\[\n\\begin{aligned}\nP(X \\text{ is even}) &= \\frac{1 + \\frac{- p}{2 - p}}{2}\\\\\n&= \\frac{1 - p}{2 - p}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2020-05-15-probability-of-even-using-probability-generating-functions/index.html#probability-generating-functions",
    "href": "posts/2020-05-15-probability-of-even-using-probability-generating-functions/index.html#probability-generating-functions",
    "title": "Probability of even/odd using probability generating functions",
    "section": "",
    "text": "A probability generating function for a discrete random variable \\(X\\) taking values \\(\\{0,1,\\ldots\\}\\) is defined as \\[\nG(z) = E[z^X] = \\sum_{j = 0}^\\infty z^j P(X = j)\n\\]\nWhich is defined for all complex \\(z\\) in which the sum converges. It always converges for \\(|z| &lt; 1\\) but the radius might be larger depending on our distribution. Clearly we have that \\(G(0) = 0\\). Also we can see that \\[\nG(1) = \\sum_{j = 0}^\\infty P(X = j) = 1\n\\] since we are summing over the entire sample space.\n\n\nThis is problem 1.11 in Ross’s Stochastic Processes Second Edition.\nWe have that \\[\n\\begin{aligned}\nG(-1) &= \\sum_{j = 0}^\\infty (-1)^j P(X = j)\\\\\n  &= P(X = 0) - P(X = 1) + P(X = 2) - \\cdots\\\\\nG(1) &= P(X = 0) + P(X = 1) + P(X = 2) + \\cdots\\\\\nG(-1) + G(1) &= 2P(X = 0) + 2P(X = 2) + 2 P(X = 4) + \\cdots\n\\end{aligned}\n\\]\nAssuming that 0 is considered even it follows that \\[\n\\begin{aligned}\nP(X \\text{ is even}) = \\frac{G(1) + G(-1)}{2} = \\frac{1 + G(-1)}{2}\n\\end{aligned}\n\\] We also have that \\[\nP(X \\text{ is odd}) = 1 - \\frac{1 + G(-1)}{2} = \\frac{1 - G(-1)}{2}\n\\]\n\n\nAssume that \\(X\\) is a binomial with parameters \\(n\\) and \\(p\\) \\[\nP(X = x) = {n \\choose x} p^x (1 - p)^{n - x}\n\\] Then we can compute the probability generating function\n\\[\n\\begin{aligned}\nG(z) = E[z^X] &= \\sum_{j = 0}^\\infty z^j P(X = j)\\\\\n  &= \\sum_{j = 0}^\\infty z^j {n \\choose j} p^j (1 - p)^{n - j}\\\\\n  &= \\sum_{j = 0}^n {n \\choose j} (zp)^j (1 - p)^{n - j}\\\\\n  &= (zp + 1 - p)^n && \\text{Binomial theorem}\n\\end{aligned}\n\\]\n\npgf_binom &lt;- function(z, n, p) {\n  (z * p + 1 - p)^n\n}\n\nz &lt;- seq(0, 1, length.out = 1000)\nn &lt;- 10\np &lt;- 1/3\nplot(z, pgf_binom(z, n, 3/4), type = \"l\")\n\nNote that the sum converges for all real \\(z\\). This is a good point to check that our result matches that \\(G(1) = 1\\). So \\(G(-1) = (1 - 2p)^n\\). Thus, \\[\nP(X \\text{ is even}) = \\frac{1 + (1 - 2p)^n}{2}\n\\]\n\n\n\nLet \\(X\\) be a Poisson random variable with mean \\(\\lambda\\) \\[\nP(X = k) = e^{-\\lambda} \\frac{\\lambda^k}{k!}\n\\]\nThen \\[\n\\begin{aligned}\nG(z) = E[z^X] &= \\sum_{j = 0}^\\infty z^j e^{-\\lambda} \\frac{\\lambda^j}{j!}\\\\\n  &= e^{-\\lambda} \\sum_{j = 0}^\\infty  \\frac{(\\lambda z)^j}{j!}\\\\\n  &= e^{-\\lambda} e^{\\lambda z}\\\\\n  &= e^{-\\lambda + \\lambda z}\n\\end{aligned}\n\\] Note that the sum converges for all real \\(z\\). Again as an exercise check that \\(G(1) = 1\\). It follows that \\[\nG(-1) = e^{-2\\lambda}\n\\] So the probability of a Poisson random variable being even is \\[\nP(X \\text{ is even}) = \\frac{1 + e^{-2\\lambda}}{2}\n\\]\n\n\n\nAssume now that \\(X\\) is geometric with parameter \\(p\\) with \\(X \\in \\{1,2,\\ldots\\}\\). \\[\nP(X = k) = p (1 - p)^{k - 1}\n\\] Then the probability generating function for \\(X\\) is\n\\[\n\\begin{aligned}\nG(z) = E[z^X] &= \\sum_{j = 1}^\\infty z^j P(X = j)\\\\\n  &= \\sum_{j = 1}^\\infty z^j p (1 - p)^{j - 1}\\\\\n  &= pz \\sum_{j = 1}^\\infty (z(1 - p))^{j - 1}\\\\\n  &= pz \\sum_{j = 0}^\\infty (z(1 - p))^{j}\\\\\n  &= \\frac{pz}{1 - z(1 - p)} && \\text{ for } |z| &lt; \\frac{1}{1 - p}\n\\end{aligned}\n\\] Since \\(1 - p &lt; 1\\), then the sum converges for \\(z = -1\\). It follows that \\[\nG(-1) = \\frac{- p}{2 - p}\n\\] So the probability that the geometric random variable \\(X\\) is even is given by\n\\[\n\\begin{aligned}\nP(X \\text{ is even}) &= \\frac{1 + \\frac{- p}{2 - p}}{2}\\\\\n&= \\frac{1 - p}{2 - p}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2019-08-31-dynamic-r-markdown-reports-with-shiny/index.html",
    "href": "posts/2019-08-31-dynamic-r-markdown-reports-with-shiny/index.html",
    "title": "Dynamic R Markdown Reports with Shiny",
    "section": "",
    "text": "TL;DR here is an example application. See the explaination below.\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  downloadButton('download')\n)\n\nserver &lt;- function(input, output) {\n  output$download &lt;- downloadHandler(\n    filename = \"listing.pdf\",\n    content = function(f) {\n      # Create a new empty environment\n      # This allows us to pass in only the relevant variables into the report\n      e &lt;- new.env()\n      # Pass two data sets into our template\n      e$datasets &lt;- list(mtcars, iris)\n      # Render the document\n      rmarkdown::render('template.Rmd',\n                        output_format = rmarkdown::pdf_document(),\n                        output_file=f,\n                        envir = e)\n    }\n  )\n}\n\nshinyApp(ui = ui, server = server)\nA common issue I have run into is dynamically generating reports. In particular, during my summer at Novartis there was lots of demands for report listings that could be generated on the fly from a Shiny application. Using the great rmarkdown package makes it really easy to do this. My approach is to create a template R markdown file and dynamically generate it using rmarkdown::render in a download handler.\nThe full application is available here: https://github.com/stefaneng/Shiny-Dynamic-Report-Generation\nWe first need a R markdown template that we can use for the report generation. We want to print out some data set using the pander package to make nicer formatted tables.\n---\ntitle: \"Example Template\"\nauthor: \"Stefan Eng\"\ndate: \"8/7/2019\"\noutput: pdf_document\n---\n\n```{r setup, include=FALSE}\nlibrary(knitr)\nlibrary(pander)\nknitr::opts_chunk$set(echo = FALSE)\n```\n\n```{r, results='asis'}\npanderOptions('knitr.auto.asis', FALSE)\nfor(d in datasets) {\n  pander::pander(d, split.table=120)\n}\n```\nTo generate the report from Shiny, we use a downloadHandler and render the template using rmarkdown::render. This is called each time the user clicks the download button. In the full example on github, the user can select a data set to include in the report which is more realistic. The example given is the simplest to get you going on generating your own dynamic reports."
  },
  {
    "objectID": "posts/2019-08-31-dynamic-r-markdown-reports-with-shiny/index.html#reproducibility",
    "href": "posts/2019-08-31-dynamic-r-markdown-reports-with-shiny/index.html#reproducibility",
    "title": "Dynamic R Markdown Reports with Shiny",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Catalina 10.15.7\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2022-12-11\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P cli           3.4.1   2022-09-23 [?] CRAN (R 4.2.0)\n P digest        0.6.31  2022-12-11 [?] CRAN (R 4.2.1)\n P evaluate      0.18    2022-11-07 [?] CRAN (R 4.2.0)\n P fastmap       1.1.0   2021-01-25 [?] CRAN (R 4.2.0)\n P glue          1.6.2   2022-02-24 [?] CRAN (R 4.2.0)\n P here        * 1.0.1   2020-12-13 [?] CRAN (R 4.2.0)\n P htmltools     0.5.4   2022-12-07 [?] CRAN (R 4.2.0)\n P htmlwidgets   1.5.4   2021-09-08 [?] CRAN (R 4.2.0)\n P jsonlite      1.8.4   2022-12-06 [?] CRAN (R 4.2.1)\n P knitr         1.41    2022-11-18 [?] CRAN (R 4.2.1)\n P lifecycle     1.0.3   2022-10-07 [?] CRAN (R 4.2.0)\n P magrittr      2.0.3   2022-03-30 [?] CRAN (R 4.2.0)\n   renv          0.16.0  2022-09-29 [1] CRAN (R 4.2.0)\n P rlang         1.0.6   2022-09-24 [?] CRAN (R 4.2.0)\n P rmarkdown     2.18    2022-11-09 [?] CRAN (R 4.2.0)\n P rprojroot     2.0.3   2022-04-02 [?] CRAN (R 4.2.0)\n P rstudioapi    0.14    2022-08-22 [?] CRAN (R 4.2.0)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.0)\n P stringi       1.7.8   2022-07-11 [?] CRAN (R 4.2.0)\n P stringr       1.5.0   2022-12-02 [?] CRAN (R 4.2.0)\n P vctrs         0.5.1   2022-11-16 [?] CRAN (R 4.2.0)\n P xfun          0.35    2022-11-16 [?] CRAN (R 4.2.0)\n P yaml          2.3.6   2022-10-18 [?] CRAN (R 4.2.0)\n\n [1] /Users/stefaneng/personal_devel/stefanengineering.comV3/renv/library/R-4.2/x86_64-apple-darwin17.0\n [2] /Users/stefaneng/personal_devel/stefanengineering.comV3/renv/sandbox/R-4.2/x86_64-apple-darwin17.0/84ba8b13\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stefanengineering.com",
    "section": "",
    "text": "Sum of uniform random variables until sum is greater than one\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nR package build\n\n\n\n\n\n\n  \n\n\n\n\nJoint distribution of sums of exponential random variables\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2020\n\n\nStefan Eng\n\n\n\n\n\n\n  \n\n\n\n\nProbability of even/odd using probability generating functions\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2020\n\n\nStefan Eng\n\n\n\n\n\n\n  \n\n\n\n\nInside a Basel Novartis Internship\n\n\n\n\n\n\n\njob\n\n\npersonal\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2019\n\n\nStefan Eng\n\n\n\n\n\n\n  \n\n\n\n\nDynamic R Markdown Reports with Shiny\n\n\n\n\n\n\n\nR\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2019\n\n\nStefan Eng\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Time Series\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2019\n\n\nStefan Eng\n\n\n\n\n\n\n  \n\n\n\n\n1990 to 1992 Census Crime Rate Prediction.\n\n\n\n\n\n\n\nR\n\n\nschool\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2019\n\n\nStefan Eng\n\n\n\n\n\n\n  \n\n\n\n\nCloze Deletion Prediction with LSTM Neural Networks\n\n\n\n\n\n\n\nmachine-learning\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2018\n\n\nStefan Eng\n\n\n\n\n\n\nNo matching items"
  }
]